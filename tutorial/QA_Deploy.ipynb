{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hands-on: Deploying Question Answering with BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-trained language representations have been shown to improve many downstream NLP tasks such as question answering, and natural language inference. Devlin, Jacob, et al proposed BERT [1] (Bidirectional Encoder Representations from Transformers), which fine-tunes deep bidirectional representations on a wide range of tasks with minimal task-specific parameters, and obtained state- of-the-art results.\n",
    "\n",
    "After finishing training QA with BERT (the previous notebook \"QA_Training.ipydb\"), let us load a trained model to perform inference on the SQuAD dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A quick overview: an example from SQuAD dataset is like below:\n",
    "\n",
    "    (2, \n",
    "    '56be4db0acb8001400a502ee', \n",
    "    'Where did Super Bowl 50 take place?', \n",
    "\n",
    "    'Super Bowl 50 was an American football game to determine the champion of the National \n",
    "    Football League (NFL) for the 2015 season. The American Football Conference (AFC) \n",
    "    champion Denver Broncos defeated the National Football Conference (NFC) champion \n",
    "    Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played \n",
    "    on February 7, 2016, at Levi\\'s Stadium in the San Francisco Bay Area at Santa Clara, \n",
    "    California. As this was the 50th Super Bowl, the league emphasized the \"golden \n",
    "    anniversary\" with various gold-themed initiatives, as well as temporarily suspending \n",
    "    the tradition of naming each Super Bowl game with Roman numerals (under which the \n",
    "    game would have been known as \"Super Bowl L\"), so that the logo could prominently \n",
    "    feature the Arabic numerals 50.', \n",
    "\n",
    "    ['Santa Clara, California', \"Levi's Stadium\", \"Levi's Stadium \n",
    "    in the San Francisco Bay Area at Santa Clara, California.\"], \n",
    "\n",
    "    [403, 355, 355])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy on SageMaker\n",
    "\n",
    "1. Preparing functions for inference \n",
    "2. Saving the model parameters\n",
    "3. Building a docker container with dependencies installed\n",
    "4. Launching a serving end-point with SageMaker SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preparing functions for inference\n",
    "\n",
    "Two functions: \n",
    "1. ```model_fn``` to load model parameters\n",
    "2. ```transform_fn(``` to run model inference given an input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile code/serve.py\n",
    "import collections, json, logging, warnings\n",
    "import multiprocessing as mp\n",
    "from functools import partial\n",
    "\n",
    "import gluonnlp as nlp\n",
    "import mxnet as mx\n",
    "from mxnet.gluon import Block, nn\n",
    "from bert.data.qa import preprocess_dataset, SQuADTransform\n",
    "import bert.bert_qa_evaluate\n",
    "\n",
    "\n",
    "def model_fn(params_path, model_dir = \"\"):\n",
    "    \"\"\"\n",
    "    Load the gluon model. Called once when hosting service starts.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        params_path: filename of trained BERT model weights, \n",
    "            e.g., params_path = \"bert_qa-7eb11865.params\"\n",
    "        model_dir: The directory where model files are stored\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        net: a Gluon model,\n",
    "        vocab: the BERT vocabulary,\n",
    "        transform: a SQuADTransform\n",
    "    \"\"\"\n",
    "    bert_model, vocab = nlp.model.get_model('bert_12_768_12',\n",
    "                                        dataset_name='book_corpus_wiki_en_uncased',\n",
    "                                        use_classifier=False,\n",
    "                                        use_decoder=False,\n",
    "                                        use_pooler=False,\n",
    "                                        pretrained=False)\n",
    "    net = bert_qa_evaluate.BertForQA(bert_model)\n",
    "    if len(model_dir) > 0:\n",
    "        params_path = model_dir + \"/\" +params_path\n",
    "    net.load_parameters(params_path, ctx=mx.cpu())\n",
    "    \n",
    "    tokenizer = nlp.data.BERTTokenizer(vocab,  lower=True)\n",
    "    transform = SQuADTransform(tokenizer, is_pad=False, is_training=False, do_lookup=False)\n",
    "    return net, vocab, transform\n",
    "\n",
    "\n",
    "def transform_fn(model, input_data, input_content_type=None, output_content_type=None):\n",
    "    \"\"\"\n",
    "    Transform a request using the Gluon model. Called once per request.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        model: The Gluon model from model_fn()\n",
    "        input_data: The input data, will be a list(tuples) here\n",
    "            Example:\n",
    "            ## (example_id, [question, content], ques_cont_token_types, valid_length, _, _)\n",
    "\n",
    "        input_content_type: The request content type, assume json\n",
    "        output_content_type: The (desired) response content type, assume json\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        response payload and output content type.\n",
    "    \"\"\"\n",
    "    net, vocab, squadTransform = model\n",
    "    data = json.loads(input_data)\n",
    "    test_examples_tuples = bert_qa_evaluate._test_example_transform(data)\n",
    "    test_dataset = mx.gluon.data.SimpleDataset(test_examples_tuples)\n",
    "    all_results = bert_qa_evaluate.get_all_results(net, vocab, squadTransform, test_dataset, ctx=mx.cpu())\n",
    "    all_predictions = collections.defaultdict(list)\n",
    "    data_transform = test_dataset.transform(squadTransform._transform)\n",
    "    for features in data_transform:\n",
    "        f_id = features[0].example_id\n",
    "        results = all_results[f_id]\n",
    "        prediction, nbest = bert_qa_evaluate.predict(\n",
    "            features=features,\n",
    "            results=results,\n",
    "            tokenizer=nlp.data.BERTBasicTokenizer(vocab))        \n",
    "        nbest_prediction = [] \n",
    "        for i in range(3):\n",
    "            nbest_prediction.append('%.2f%% \\t %s'%(nbest[i][1] * 100, nbest[i][0]))\n",
    "        all_predictions[f_id] = nbest_prediction\n",
    "    response_body = json.dumps(all_predictions)\n",
    "    return response_body, output_content_type\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Saving the model parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to zip the BERT model parameters, vocabulary file, and all the inference files (```code/serve.py```, ```bert/data/qa.py```, ```bert_qa_evaluate.py```) to a ```model.tar.gz``` file. (Note that the ```serve.py``` is the \"entry_point\" for Sagemaker to do the inference, and it needs to be under ```code/``` directory.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "with tarfile.open(\"model.tar.gz\", \"w:gz\") as tar:\n",
    "    tar.add(\"code/serve.py\")\n",
    "    tar.add(\"bert/data/qa.py\")\n",
    "    tar.add(\"bert_qa_evaluate.py\")\n",
    "    tar.add(\"bert_qa-7eb11865.params\")\n",
    "    tar.add(\"vocab.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Building a docker container with dependencies installed\n",
    "\n",
    "Let's prepare a docker container with all the dependencies required for model inference. Here we build a docker container based on the SageMaker MXNet inference container, and you can find the list of all available inference containers at https://docs.aws.amazon.com/sagemaker/latest/dg/pre-built-containers-frameworks-deep-learning.html\n",
    "\n",
    "Here we use local mode for demonstration purpose. To deploy on actual instances, you need to login into AWS elastic container registry (ECR) service, and push the container to ECR. \n",
    "\n",
    "```\n",
    "docker build -t $YOUR_EDR_DOCKER_TAG . -f Dockerfile\n",
    "$(aws ecr get-login --no-include-email --region $YOUR_REGION)\n",
    "docker push $YOUR_EDR_DOCKER_TAG\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile Dockerfile\n",
    "\n",
    "ARG REGION\n",
    "FROM 763104351884.dkr.ecr.$REGION.amazonaws.com/mxnet-inference:1.6.0-gpu-py3\n",
    "\n",
    "RUN pip install --upgrade --user --pre 'mxnet-mkl' 'https://github.com/dmlc/gluon-nlp/tarball/v0.9.x'\n",
    "RUN pip list | grep mxnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  844.4MB\n",
      "Step 1/4 : ARG REGION\n",
      "Step 2/4 : FROM 763104351884.dkr.ecr.$REGION.amazonaws.com/mxnet-inference:1.6.0-gpu-py3\n",
      " ---> f9c75f96f647\n",
      "Step 3/4 : RUN pip install --upgrade --user --pre 'mxnet-mkl' 'https://github.com/dmlc/gluon-nlp/tarball/v0.9.x'\n",
      " ---> Running in 517c633ca1c2\n",
      "Collecting https://github.com/dmlc/gluon-nlp/tarball/v0.9.x\n",
      "  Downloading https://github.com/dmlc/gluon-nlp/tarball/v0.9.x\n",
      "Collecting mxnet-mkl\n",
      "  Downloading https://files.pythonhosted.org/packages/64/72/c5566aabde6ee0bda1f09d026603169a717dbd9f26f6be85ee2b4ed2cf03/mxnet_mkl-1.6.0b20191025-py2.py3-none-manylinux1_x86_64.whl (64.9MB)\n",
      "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/site-packages (from gluonnlp==0.9.0.dev0) (1.17.4)\n",
      "Requirement already satisfied, skipping upgrade: requests<3,>=2.20.0 in /usr/local/lib/python3.6/site-packages (from mxnet-mkl) (2.22.0)\n",
      "Requirement already satisfied, skipping upgrade: graphviz<0.9.0,>=0.8.1 in /usr/local/lib/python3.6/site-packages (from mxnet-mkl) (0.8.4)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/site-packages (from requests<3,>=2.20.0->mxnet-mkl) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/site-packages (from requests<3,>=2.20.0->mxnet-mkl) (2019.9.11)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/site-packages (from requests<3,>=2.20.0->mxnet-mkl) (1.25.7)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /usr/local/lib/python3.6/site-packages (from requests<3,>=2.20.0->mxnet-mkl) (2.8)\n",
      "Installing collected packages: mxnet-mkl, gluonnlp\n",
      "    Running setup.py install for gluonnlp: started\n",
      "    Running setup.py install for gluonnlp: finished with status 'done'\n",
      "Successfully installed gluonnlp-0.9.0.dev0 mxnet-mkl-1.6.0b20191025\n",
      "Removing intermediate container 517c633ca1c2\n",
      " ---> b4b705e287b7\n",
      "Step 4/4 : RUN pip list | grep mxnet\n",
      " ---> Running in 17acccdd7c01\n",
      "aws-mxnet-cu101mkl                1.6.0rc0      \n",
      "keras-mxnet                       2.2.4.1       \n",
      "mxnet-mkl                         1.6.0b20191025\n",
      "mxnet-model-server                1.0.8         \n",
      "sagemaker-mxnet-serving-container 1.1.6.dev0    \n",
      "Removing intermediate container 17acccdd7c01\n",
      " ---> 4f3e6151d41c\n",
      "Successfully built 4f3e6151d41c\n",
      "Successfully tagged my-docker:inference\n"
     ]
    }
   ],
   "source": [
    "!export REGION=$(wget -qO- http://169.254.169.254/latest/meta-data/placement/availability-zone) &&\\\n",
    " docker build --no-cache --build-arg REGION=${REGION::-1} -t my-docker:inference . -f Dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Launching a serving end-point with SageMaker SDK\n",
    "\n",
    "We create a MXNet model which can be deployed later, by specifying the docker image, and entry point for the inference code. If ```serve.py``` does not work, use ```dummy_hosting_module.py``` for debugging purpose. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.mxnet.model import MXNetModel\n",
    "sagemaker_model = MXNetModel(model_data='file:///home/ec2-user/SageMaker/ako2020-bert/tutorial/model.tar.gz',\n",
    "                             image='my-docker:inference', # docker images\n",
    "                             role=sagemaker.get_execution_role(), \n",
    "                             py_version='py3',            # python version\n",
    "                             entry_point='serve.py',\n",
    "                             source_dir='.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use 'local' mode to test our deployment code, where the inference happens on the current instance.\n",
    "If you are ready to deploy the model on a new instance, change the `instance_type` argument to values such as `ml.c4.xlarge`.\n",
    "\n",
    "Here we use 'local' mode for testing, for real instances use c5.2xlarge, p2.xlarge, etc. **The following line will start docker container building.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attaching to tmpmil27owm_algo-1-k003q_1\n",
      "\u001b[36malgo-1-k003q_1  |\u001b[0m 2020-01-15 07:35:17,400 [INFO ] main com.amazonaws.ml.mms.ModelServer - \n",
      "\u001b[36malgo-1-k003q_1  |\u001b[0m MMS Home: /usr/local/lib/python3.6/site-packages\n",
      "\u001b[36malgo-1-k003q_1  |\u001b[0m Current directory: /\n",
      "\u001b[36malgo-1-k003q_1  |\u001b[0m Temp directory: /home/model-server/tmp\n",
      "\u001b[36malgo-1-k003q_1  |\u001b[0m Number of GPUs: 0\n",
      "\u001b[36malgo-1-k003q_1  |\u001b[0m Number of CPUs: 8\n",
      "\u001b[36malgo-1-k003q_1  |\u001b[0m Max heap size: 13646 M\n",
      "\u001b[36malgo-1-k003q_1  |\u001b[0m Python executable: /usr/local/bin/python3.6\n",
      "\u001b[36malgo-1-k003q_1  |\u001b[0m Config file: /etc/sagemaker-mms.properties\n",
      "\u001b[36malgo-1-k003q_1  |\u001b[0m Inference address: http://0.0.0.0:8080\n",
      "\u001b[36malgo-1-k003q_1  |\u001b[0m Management address: http://0.0.0.0:8080\n",
      "\u001b[36malgo-1-k003q_1  |\u001b[0m Model Store: /.sagemaker/mms/models\n",
      "\u001b[36malgo-1-k003q_1  |\u001b[0m Initial Models: ALL\n",
      "\u001b[36malgo-1-k003q_1  |\u001b[0m Log dir: /logs\n",
      "\u001b[36malgo-1-k003q_1  |\u001b[0m Metrics dir: /logs\n",
      "\u001b[36malgo-1-k003q_1  |\u001b[0m Netty threads: 0\n",
      "\u001b[36malgo-1-k003q_1  |\u001b[0m Netty client threads: 0\n",
      "\u001b[36malgo-1-k003q_1  |\u001b[0m Default workers per model: 8\n",
      "\u001b[36malgo-1-k003q_1  |\u001b[0m Blacklist Regex: N/A\n",
      "\u001b[36malgo-1-k003q_1  |\u001b[0m Maximum Response Size: 6553500\n",
      "\u001b[36malgo-1-k003q_1  |\u001b[0m Maximum Request Size: 6553500\n",
      "\u001b[36malgo-1-k003q_1  |\u001b[0m 2020-01-15 07:35:17,462 [INFO ] main com.amazonaws.ml.mms.wlm.ModelManager - Model model loaded.\n",
      "\u001b[36malgo-1-k003q_1  |\u001b[0m 2020-01-15 07:35:17,482 [INFO ] main com.amazonaws.ml.mms.ModelServer - Initialize Inference server with: EpollServerSocketChannel.\n",
      "\u001b[36malgo-1-k003q_1  |\u001b[0m 2020-01-15 07:35:17,719 [INFO ] W-9007-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.mms.sock.9007\n",
      "\u001b[36malgo-1-k003q_1  |\u001b[0m 2020-01-15 07:35:17,719 [INFO ] W-9007-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - [PID]81\n",
      "\u001b[36malgo-1-k003q_1  |\u001b[0m 2020-01-15 07:35:17,721 [INFO ] W-9001-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.mms.sock.9001\n",
      "\u001b[36malgo-1-k003q_1  |\u001b[0m 2020-01-15 07:35:17,721 [INFO ] W-9001-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - [PID]76\n",
      "\u001b[36malgo-1-k003q_1  |\u001b[0m 2020-01-15 07:35:17,726 [INFO ] W-9002-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.mms.sock.9002\n",
      "\u001b[36malgo-1-k003q_1  |\u001b[0m 2020-01-15 07:35:17,727 [INFO ] W-9002-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - [PID]77\n",
      "\u001b[36malgo-1-k003q_1  |\u001b[0m 2020-01-15 07:35:17,727 [INFO ] W-9007-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - MXNet worker started.\n",
      "\u001b[36malgo-1-k003q_1  |\u001b[0m 2020-01-15 07:35:17,728 [INFO ] W-9007-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Python runtime: 3.6.8\n",
      "\u001b[36malgo-1-k003q_1  |\u001b[0m 2020-01-15 07:35:17,729 [INFO ] W-9002-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - MXNet worker started.\n",
      "\u001b[36malgo-1-k003q_1  |\u001b[0m 2020-01-15 07:35:17,730 [INFO ] W-9002-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Python runtime: 3.6.8\n",
      "\u001b[36malgo-1-k003q_1  |\u001b[0m 2020-01-15 07:35:17,731 [INFO ] W-9006-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.mms.sock.9006\n",
      "\u001b[36malgo-1-k003q_1  |\u001b[0m 2020-01-15 07:35:17,730 [INFO ] W-9002-model com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.mms.sock.9002\n",
      "\u001b[36malgo-1-k003q_1  |\u001b[0m 2020-01-15 07:35:17,731 [INFO ] W-9006-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - [PID]74\n",
      "\u001b[36malgo-1-k003q_1  |\u001b[0m 2020-01-15 07:35:17,731 [INFO ] W-9007-model com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.mms.sock.9007\n",
      "\u001b[36malgo-1-k003q_1  |\u001b[0m 2020-01-15 07:35:17,732 [INFO ] W-9001-model com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.mms.sock.9001\n",
      "\u001b[36malgo-1-k003q_1  |\u001b[0m 2020-01-15 07:35:17,732 [INFO ] W-9001-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - MXNet worker started.\n",
      "\u001b[36malgo-1-k003q_1  |\u001b[0m 2020-01-15 07:35:17,732 [INFO ] W-9001-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Python runtime: 3.6.8\n",
      "\u001b[36malgo-1-k003q_1  |\u001b[0m 2020-01-15 07:35:17,732 [INFO ] W-9006-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - MXNet worker started.\n",
      "\u001b[36malgo-1-k003q_1  |\u001b[0m 2020-01-15 07:35:17,733 [INFO ] W-9006-model com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.mms.sock.9006\n",
      "\u001b[36malgo-1-k003q_1  |\u001b[0m 2020-01-15 07:35:17,733 [INFO ] W-9006-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Python runtime: 3.6.8\n",
      "\u001b[36malgo-1-k003q_1  |\u001b[0m 2020-01-15 07:35:17,738 [INFO ] W-9003-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.mms.sock.9003\n",
      "\u001b[36malgo-1-k003q_1  |\u001b[0m 2020-01-15 07:35:17,738 [INFO ] W-9003-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - [PID]78\n",
      "\u001b[36malgo-1-k003q_1  |\u001b[0m 2020-01-15 07:35:17,739 [INFO ] main com.amazonaws.ml.mms.ModelServer - Inference API bind to: http://0.0.0.0:8080\n",
      "\u001b[36malgo-1-k003q_1  |\u001b[0m 2020-01-15 07:35:17,740 [INFO ] W-9003-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - MXNet worker started.\n",
      "\u001b[36malgo-1-k003q_1  |\u001b[0m 2020-01-15 07:35:17,740 [INFO ] W-9003-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Python runtime: 3.6.8\n",
      "\u001b[36malgo-1-k003q_1  |\u001b[0m 2020-01-15 07:35:17,740 [INFO ] W-9003-model com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.mms.sock.9003\n",
      "\u001b[36malgo-1-k003q_1  |\u001b[0m 2020-01-15 07:35:17,742 [INFO ] W-9004-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.mms.sock.9004\n",
      "\u001b[36malgo-1-k003q_1  |\u001b[0m 2020-01-15 07:35:17,742 [INFO ] W-9004-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - [PID]80\n",
      "\u001b[36malgo-1-k003q_1  |\u001b[0m 2020-01-15 07:35:17,744 [INFO ] W-9004-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - MXNet worker started.\n",
      "\u001b[36malgo-1-k003q_1  |\u001b[0m 2020-01-15 07:35:17,744 [INFO ] W-9004-model com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.mms.sock.9004\n",
      "\u001b[36malgo-1-k003q_1  |\u001b[0m 2020-01-15 07:35:17,744 [INFO ] W-9004-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Python runtime: 3.6.8\n",
      "\u001b[36malgo-1-k003q_1  |\u001b[0m Model server started.\n",
      "\u001b[36malgo-1-k003q_1  |\u001b[0m 2020-01-15 07:35:17,749 [INFO ] W-9007-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.mms.sock.9007.\n",
      "\u001b[36malgo-1-k003q_1  |\u001b[0m 2020-01-15 07:35:17,749 [INFO ] W-9001-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.mms.sock.9001.\n",
      "\u001b[36malgo-1-k003q_1  |\u001b[0m 2020-01-15 07:35:17,749 [INFO ] W-9006-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.mms.sock.9006.\n",
      "\u001b[36malgo-1-k003q_1  |\u001b[0m 2020-01-15 07:35:17,750 [INFO ] W-9003-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.mms.sock.9003.\n",
      "\u001b[36malgo-1-k003q_1  |\u001b[0m 2020-01-15 07:35:17,749 [INFO ] W-9004-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.mms.sock.9004.\n",
      "\u001b[36malgo-1-k003q_1  |\u001b[0m 2020-01-15 07:35:17,754 [INFO ] W-9002-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.mms.sock.9002.\n",
      "\u001b[36malgo-1-k003q_1  |\u001b[0m 2020-01-15 07:35:17,766 [WARN ] pool-2-thread-1 com.amazonaws.ml.mms.metrics.MetricCollector - worker pid is not available yet.\n",
      "\u001b[36malgo-1-k003q_1  |\u001b[0m 2020-01-15 07:35:17,772 [INFO ] W-9005-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.mms.sock.9005\n",
      "\u001b[36malgo-1-k003q_1  |\u001b[0m 2020-01-15 07:35:17,772 [INFO ] W-9005-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - [PID]75\n",
      "\u001b[36malgo-1-k003q_1  |\u001b[0m 2020-01-15 07:35:17,773 [INFO ] W-9005-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - MXNet worker started.\n",
      "\u001b[36malgo-1-k003q_1  |\u001b[0m 2020-01-15 07:35:17,773 [INFO ] W-9005-model com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.mms.sock.9005\n",
      "\u001b[36malgo-1-k003q_1  |\u001b[0m 2020-01-15 07:35:17,773 [INFO ] W-9005-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Python runtime: 3.6.8\n",
      "\u001b[36malgo-1-k003q_1  |\u001b[0m 2020-01-15 07:35:17,792 [INFO ] W-9005-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.mms.sock.9005.\n",
      "\u001b[36malgo-1-k003q_1  |\u001b[0m 2020-01-15 07:35:17,818 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.mms.sock.9000\n",
      "\u001b[36malgo-1-k003q_1  |\u001b[0m 2020-01-15 07:35:17,819 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - [PID]79\n",
      "\u001b[36malgo-1-k003q_1  |\u001b[0m 2020-01-15 07:35:17,819 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - MXNet worker started.\n",
      "\u001b[36malgo-1-k003q_1  |\u001b[0m 2020-01-15 07:35:17,819 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.mms.sock.9000\n",
      "\u001b[36malgo-1-k003q_1  |\u001b[0m 2020-01-15 07:35:17,819 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Python runtime: 3.6.8\n",
      "\u001b[36malgo-1-k003q_1  |\u001b[0m 2020-01-15 07:35:17,840 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.mms.sock.9000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36malgo-1-k003q_1  |\u001b[0m 2020-01-15 07:35:20,812 [INFO ] W-9007-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 3029\n",
      "\u001b[36malgo-1-k003q_1  |\u001b[0m 2020-01-15 07:35:20,821 [INFO ] W-9004-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 3042\n",
      "\u001b[36malgo-1-k003q_1  |\u001b[0m 2020-01-15 07:35:20,838 [INFO ] W-9006-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 3035\n",
      "\u001b[36malgo-1-k003q_1  |\u001b[0m 2020-01-15 07:35:20,843 [INFO ] W-9003-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 3060\n",
      "\u001b[36malgo-1-k003q_1  |\u001b[0m 2020-01-15 07:35:20,849 [INFO ] W-9002-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 3057\n",
      "\u001b[36malgo-1-k003q_1  |\u001b[0m 2020-01-15 07:35:20,879 [INFO ] W-9001-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 3088\n",
      "\u001b[36malgo-1-k003q_1  |\u001b[0m 2020-01-15 07:35:20,910 [INFO ] W-9005-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 3120\n",
      "\u001b[36malgo-1-k003q_1  |\u001b[0m 2020-01-15 07:35:20,943 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 3102\n",
      "\u001b[36malgo-1-k003q_1  |\u001b[0m 2020-01-15 07:35:22,479 [INFO ] pool-1-thread-9 ACCESS_LOG - /172.18.0.1:34334 \"GET /ping HTTP/1.1\" 200 12\n",
      "!"
     ]
    }
   ],
   "source": [
    "predictor = sagemaker_model.deploy(initial_instance_count=1, instance_type='local')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us try to submit a inference job. Here we simply grab two datapoints from the SQuAD dataset and pass the examples to our predictor by calling ```predictor.predict```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36malgo-1-k003q_1  |\u001b[0m 2020-01-15 07:35:22,535 [INFO ] W-9007-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Vocab file is not found. Downloading.\n",
      "\u001b[36malgo-1-k003q_1  |\u001b[0m 2020-01-15 07:35:22,536 [INFO ] W-9007-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Downloading /root/.mxnet/models/1579073722.5359378book_corpus_wiki_en_uncased-a6607397.zip from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/vocab/book_corpus_wiki_en_uncased-a6607397.zip...\n",
      "\u001b[36malgo-1-k003q_1  |\u001b[0m 2020-01-15 07:35:23,904 [INFO ] W-9007-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Done! Transform dataset costs 0.56 seconds.\n",
      "\u001b[36malgo-1-k003q_1  |\u001b[0m 2020-01-15 07:35:25,676 [INFO ] W-9007-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 3175\n",
      "\u001b[36malgo-1-k003q_1  |\u001b[0m 2020-01-15 07:35:25,677 [INFO ] W-9007-model ACCESS_LOG - /172.18.0.1:34338 \"POST /invocations HTTP/1.1\" 200 3179\n"
     ]
    }
   ],
   "source": [
    "## test\n",
    "my_test_example_0 = ('Which NFL team represented the AFC at Super Bowl 50?',\n",
    " 'Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi\\'s Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.')\n",
    "\n",
    "my_test_example_1 = ('Where did Super Bowl 50 take place?',\n",
    " 'Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi\\'s Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.')\n",
    "\n",
    "my_test_examples = (my_test_example_0, my_test_example_1)\n",
    "\n",
    "# mymodel = model_fn(params_path = \"bert_qa-7eb11865.params\")\n",
    "# transform_fn(mymodel, my_test_examples)\n",
    "output = predictor.predict(my_test_examples)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction output: \n",
      "\n",
      "\n",
      "['99.36% \\t Denver Broncos', '0.23% \\t The American Football Conference (AFC) champion Denver Broncos', '0.20% \\t Broncos']\n",
      "\n",
      "\n",
      "[\"25.86% \\t Levi's Stadium in the San Francisco Bay Area at Santa Clara, California\", \"23.11% \\t Levi's Stadium\", '17.88% \\t San Francisco Bay Area at Santa Clara, California']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nPrediction output: \\n\\n\")\n",
    "\n",
    "for k in output.keys():\n",
    "    print('{}\\n\\n'.format(output[k]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Up\n",
    "\n",
    "Remove the endpoint after we are done. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gracefully stopping... (press Ctrl+C again to force)\n"
     ]
    }
   ],
   "source": [
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
