{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hands-on: Training and deploying Question Answering with BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-trained language representations have been shown to improve many downstream NLP tasks such as question answering, and natural language inference. Devlin, Jacob, et al proposed BERT [1] (Bidirectional Encoder Representations from Transformers), which fine-tunes deep bidirectional representations on a wide range of tasks with minimal task-specific parameters, and obtained state- of-the-art results.\n",
    "\n",
    "In this tutorial, we will focus on adapting the BERT model for the question answering task on the SQuAD dataset. Specifically, we will:\n",
    "\n",
    "- understand how to pre-process the SQuAD dataset to leverage the learnt representation in BERT,\n",
    "- adapt the BERT model to the question answering task, and\n",
    "- load a trained model to perform inference on the SQuAD dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sagemaker configuration\n",
    "\n",
    "This notebook requires mxnet-cu101 >= 1.6.0b20191102, gluonnlp >= 0.8.1\n",
    "We can create a sagemaker notebook instance with the lifecycle configuration file: sagemaker-lifecycle.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# One time script\n",
    "!bash sagemaker-lifecycle.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keras-mxnet                        2.2.4.2       \n",
      "mxnet-cu101                        1.6.0b20191122\n",
      "mxnet-model-server                 1.0.5         \n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "gluonnlp                           0.9.0.dev0    \n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip list | grep mxnet\n",
    "!pip list | grep gluonnlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Load MXNet and GluonNLP\n",
    "\n",
    "We first import the libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse, collections, time, logging\n",
    "import json\n",
    "import os\n",
    "import io\n",
    "import copy\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import gluonnlp as nlp\n",
    "import mxnet as mx\n",
    "import bert\n",
    "import qa_utils\n",
    "\n",
    "from gluonnlp.data import SQuAD\n",
    "from bert.model.qa import BertForQALoss, BertForQA\n",
    "from bert.data.qa import SQuADTransform, preprocess_dataset\n",
    "from bert.bert_qa_evaluate import get_F1_EM, predict, PredResult\n",
    "\n",
    "# Hyperparameters\n",
    "parser = argparse.ArgumentParser('BERT finetuning')\n",
    "parser.add_argument('--epochs', type=int, default=3)\n",
    "parser.add_argument('--batch_size', default=32)\n",
    "parser.add_argument('--num_epochs', default=1)\n",
    "parser.add_argument('--lr', default=5e-5)\n",
    "args = parser.parse_args([])\n",
    "\n",
    "epochs = args.epochs\n",
    "batch_size = args.batch_size\n",
    "num_epochs = args.num_epochs\n",
    "lr = args.lr\n",
    "\n",
    "# output_dir = args.output_dir\n",
    "# if not os.path.exists(output_dir):\n",
    "#     os.mkdir(output_dir)\n",
    "# test_batch_size = args.test_batch_size\n",
    "# optimizer = args.optimizer\n",
    "# accumulate = args.accumulate\n",
    "# warmup_ratio = args.warmup_ratio\n",
    "# log_interval = args.log_interval\n",
    "# max_seq_length = args.max_seq_length\n",
    "# doc_stride = args.doc_stride\n",
    "# max_query_length = args.max_query_length\n",
    "# n_best_size = args.n_best_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Inspect the SQuAD Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Then we take a look at the Stanford Question Answering Dataset (SQuAD). The dataset can be downloaded using the `nlp.data.SQuAD` API. In this tutorial, we create a small dataset with 3 samples from the SQuAD dataset for demonstration purpose.\n",
    "\n",
    "The question answering task on the SQuAD dataset is setup the following way. For each sample in the dataset, a context is provided. The context is usually a long paragraph which contains lots of information. Then a question asked based on the context. The goal is to find the text span in the context that answers the question in the sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T01:45:27.555133Z",
     "start_time": "2019-06-14T01:45:27.418706Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in the created dataset subsampled from SQuAD = 3\n"
     ]
    }
   ],
   "source": [
    "full_data = nlp.data.SQuAD(segment='dev', version='1.1')\n",
    "# loading a subset of the dev set of SQuAD\n",
    "num_target_samples = 3\n",
    "target_samples = [full_data[i] for i in range(num_target_samples)]\n",
    "dataset = mx.gluon.data.SimpleDataset(target_samples)\n",
    "print('Number of samples in the created dataset subsampled from SQuAD = %d'%len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,\n",
       " '56be4db0acb8001400a502ec',\n",
       " 'Which NFL team represented the AFC at Super Bowl 50?',\n",
       " 'Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi\\'s Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.',\n",
       " ['Denver Broncos', 'Denver Broncos', 'Denver Broncos'],\n",
       " [177, 177, 177])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_samples[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Let's take a look at a sample from the dataset. In this sample, the question is about the location of the game, with a description about the Super Bowl 50 game as the context. Note that three different answer spans are correct for this question, and they start from index 403, 355 and 355 in the context respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T01:45:27.560564Z",
     "start_time": "2019-06-14T01:45:27.557274Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Context:\n",
      "\n",
      "Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.\n"
     ]
    }
   ],
   "source": [
    "sample = dataset[2]\n",
    "\n",
    "context_idx = 3\n",
    "\n",
    "print('\\nContext:\\n')\n",
    "print(sample[context_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T01:45:27.567303Z",
     "start_time": "2019-06-14T01:45:27.562425Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question\n",
      "Where did Super Bowl 50 take place?\n",
      "\n",
      "Correct Answer Spans\n",
      "['Santa Clara, California', \"Levi's Stadium\", \"Levi's Stadium in the San Francisco Bay Area at Santa Clara, California.\"]\n",
      "\n",
      "Answer Span Start Indices:\n",
      "[403, 355, 355]\n"
     ]
    }
   ],
   "source": [
    "question_idx = 2\n",
    "answer_idx = 4\n",
    "answer_pos_idx = 5\n",
    "\n",
    "print(\"\\nQuestion\")\n",
    "print(sample[question_idx])\n",
    "print(\"\\nCorrect Answer Spans\")\n",
    "print(sample[answer_idx])\n",
    "print(\"\\nAnswer Span Start Indices:\")\n",
    "print(sample[answer_pos_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Data Pre-processing for QA with BERT\n",
    "\n",
    "Recall that during BERT pre-training, it takes a sentence pair as the input, separated by the 'SEP' special token. For SQuAD, we can feed the context-question pair as the sentence pair input. To use BERT to predict the starting and ending span of the answer, we can add a classification layer for each token in the context texts, to predict if a token is the start or the end of the answer span. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T01:30:12.299493Z",
     "start_time": "2019-06-14T01:30:12.183419Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![qa](natural_language_understanding/qa.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "In the next few code blocks, we will work on pre-processing the samples in the SQuAD dataset in the desired format with these special separators. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Pre-trained BERT Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "First, let's use the *get_model* API in GluonNLP to get the model definition for BERT, and the vocabulary used for the BERT model. Note that we discard the pooler and classifier layers used for the next sentence prediction task, as well as the decoder layers for the masked language model task during the BERT pre-training phase. These layers are not useful for predicting the starting and ending indices of the answer span.\n",
    "\n",
    "The list of pre-trained BERT models available in GluonNLP can be found [here](http://gluon-nlp.mxnet.io/model_zoo/bert/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T01:45:27.715444Z",
     "start_time": "2019-06-14T01:45:27.569118Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "bert_model, vocab = nlp.model.get_model('bert_12_768_12',\n",
    "                                        dataset_name='book_corpus_wiki_en_uncased',\n",
    "                                        use_classifier=False,\n",
    "                                        use_decoder=False,\n",
    "                                        use_pooler=False,\n",
    "                                        pretrained=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Note that there are several special tokens in the vocabulary for BERT. In particular, the `[SEP]` token is used for separating the sentence pairs, and the `[CLS]` token is added at the beginning of the sentence pairs. They will be used to pre-process the SQuAD dataset later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T01:45:27.720137Z",
     "start_time": "2019-06-14T01:45:27.717192Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab(size=30522, unk=\"[UNK]\", reserved=\"['[CLS]', '[SEP]', '[MASK]', '[PAD]']\")\n"
     ]
    }
   ],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The second step is to process the samples using the same tokenizer used for BERT, which is provided as the `BERTTokenizer` API in GluonNLP. Note that instead of word level and character level representation, BERT uses subwords to represent a word, separated `##`. \n",
    "\n",
    "In the following example, the word `suspending` is tokenized as two subwords (`suspend` and `##ing`), and `numerals` is tokenized as three subwords (`nu`, `##meral`, `##s`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T01:45:27.731724Z",
     "start_time": "2019-06-14T01:45:27.721690Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['as',\n",
       " 'well',\n",
       " 'as',\n",
       " 'temporarily',\n",
       " 'suspend',\n",
       " '##ing',\n",
       " 'the',\n",
       " 'tradition',\n",
       " 'of',\n",
       " 'naming',\n",
       " 'each',\n",
       " 'super',\n",
       " 'bowl',\n",
       " 'game',\n",
       " 'with',\n",
       " 'roman',\n",
       " 'nu',\n",
       " '##meral',\n",
       " '##s']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = nlp.data.BERTTokenizer(vocab=vocab, lower=True)\n",
    "\n",
    "tokenizer(\"as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Sentence Pair Composition\n",
    "\n",
    "With the tokenizer inplace, we are ready to process the question-context texts and compose sentence pairs. The functionality is available via the `SQuADTransform` API. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T01:45:27.897684Z",
     "start_time": "2019-06-14T01:45:27.734029Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Transform dataset costs 0.18 seconds.\n"
     ]
    }
   ],
   "source": [
    "transform = bert.data.qa.SQuADTransform(tokenizer, is_pad=False, is_training=False, do_lookup=False)\n",
    "dev_data_transform, _ = preprocess_dataset(dataset, transform)\n",
    "logging.info('The number of examples after preprocessing:{}'.format(len(dev_data_transform)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's take a look at the sample after the transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T01:45:27.904353Z",
     "start_time": "2019-06-14T01:45:27.899992Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "segment type: \n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "text length: 168\n",
      "\n",
      "sentence pair: \n",
      "['[CLS]', 'where', 'did', 'super', 'bowl', '50', 'take', 'place', '?', '[SEP]', 'super', 'bowl', '50', 'was', 'an', 'american', 'football', 'game', 'to', 'determine', 'the', 'champion', 'of', 'the', 'national', 'football', 'league', '(', 'nfl', ')', 'for', 'the', '2015', 'season', '.', 'the', 'american', 'football', 'conference', '(', 'afc', ')', 'champion', 'denver', 'broncos', 'defeated', 'the', 'national', 'football', 'conference', '(', 'nfc', ')', 'champion', 'carolina', 'panthers', '24', '–', '10', 'to', 'earn', 'their', 'third', 'super', 'bowl', 'title', '.', 'the', 'game', 'was', 'played', 'on', 'february', '7', ',', '2016', ',', 'at', 'levi', \"'\", 's', 'stadium', 'in', 'the', 'san', 'francisco', 'bay', 'area', 'at', 'santa', 'clara', ',', 'california', '.', 'as', 'this', 'was', 'the', '50th', 'super', 'bowl', ',', 'the', 'league', 'emphasized', 'the', '\"', 'golden', 'anniversary', '\"', 'with', 'various', 'gold', '-', 'themed', 'initiatives', ',', 'as', 'well', 'as', 'temporarily', 'suspend', '##ing', 'the', 'tradition', 'of', 'naming', 'each', 'super', 'bowl', 'game', 'with', 'roman', 'nu', '##meral', '##s', '(', 'under', 'which', 'the', 'game', 'would', 'have', 'been', 'known', 'as', '\"', 'super', 'bowl', 'l', '\"', ')', ',', 'so', 'that', 'the', 'logo', 'could', 'prominently', 'feature', 'the', 'arabic', 'nu', '##meral', '##s', '50', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "sample = dev_data_transform[2]\n",
    "print('\\nsegment type: \\n' + str(sample[2]))\n",
    "print('\\ntext length: ' + str(sample[3]))\n",
    "print('\\nsentence pair: \\n' + str(sample[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Vocabulary Lookup\n",
    "\n",
    "Finally, we convert the transformed texts to subword indices, which are used to contructor NDArrays as the inputs to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T01:45:27.910853Z",
     "start_time": "2019-06-14T01:45:27.906127Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 2073, 2106, 3565, 4605, 2753, 2202, 2173, 1029, 3, 3565, 4605, 2753, 2001, 2019, 2137, 2374, 2208, 2000, 5646, 1996, 3410, 1997, 1996, 2120, 2374, 2223, 1006, 5088, 1007, 2005, 1996, 2325, 2161, 1012, 1996, 2137, 2374, 3034, 1006, 10511, 1007, 3410, 7573, 14169, 3249, 1996, 2120, 2374, 3034, 1006, 22309, 1007, 3410, 3792, 12915, 2484, 1516, 2184, 2000, 7796, 2037, 2353, 3565, 4605, 2516, 1012, 1996, 2208, 2001, 2209, 2006, 2337, 1021, 1010, 2355, 1010, 2012, 11902, 1005, 1055, 3346, 1999, 1996, 2624, 3799, 3016, 2181, 2012, 4203, 10254, 1010, 2662, 1012, 2004, 2023, 2001, 1996, 12951, 3565, 4605, 1010, 1996, 2223, 13155, 1996, 1000, 3585, 5315, 1000, 2007, 2536, 2751, 1011, 11773, 11107, 1010, 2004, 2092, 2004, 8184, 28324, 2075, 1996, 4535, 1997, 10324, 2169, 3565, 4605, 2208, 2007, 3142, 16371, 28990, 2015, 1006, 2104, 2029, 1996, 2208, 2052, 2031, 2042, 2124, 2004, 1000, 3565, 4605, 1048, 1000, 1007, 1010, 2061, 2008, 1996, 8154, 2071, 14500, 3444, 1996, 5640, 16371, 28990, 2015, 2753, 1012, 3]\n"
     ]
    }
   ],
   "source": [
    "def vocab_lookup(example_id, subwords, type_ids, length, start, end):\n",
    "    indices = vocab[subwords]\n",
    "    return example_id, indices, type_ids, length, start, end\n",
    "\n",
    "dev_data_transform = dev_data_transform.transform(vocab_lookup, lazy=False)\n",
    "print(dev_data_transform[2][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model Definition\n",
    "\n",
    "After the data is processed, we can define the model that uses the representation produced by BERT for predicting the starting and ending positions of the answer span."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We download a BERT model trained on the SQuAD dataset, prepare the dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T01:45:32.221383Z",
     "start_time": "2019-06-14T01:45:27.922825Z"
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded checkpoint to ./bert_qa-7eb11865.params\n"
     ]
    }
   ],
   "source": [
    "net = BertForQA(bert_model)\n",
    "ctx = mx.gpu(0)\n",
    "ckpt = qa_utils.download_qa_ckpt()\n",
    "net.load_parameters(ckpt, ctx=ctx)\n",
    "\n",
    "batch_size = 1\n",
    "dev_dataloader = mx.gluon.data.DataLoader(\n",
    "    dev_data_transform, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T01:45:32.575976Z",
     "start_time": "2019-06-14T01:45:32.223336Z"
    }
   },
   "outputs": [],
   "source": [
    "all_results = collections.defaultdict(list)\n",
    "\n",
    "total_num = 0\n",
    "for data in dev_dataloader:\n",
    "    example_ids, inputs, token_types, valid_length, _, _ = data\n",
    "    total_num += len(inputs)\n",
    "    batch_size = inputs.shape[0]\n",
    "    output = net(inputs.astype('float32').as_in_context(ctx),\n",
    "                               token_types.astype('float32').as_in_context(ctx),\n",
    "                               valid_length.astype('float32').as_in_context(ctx))\n",
    "    pred_start, pred_end = mx.nd.split(output, axis=2, num_outputs=2)\n",
    "    example_ids = example_ids.asnumpy().tolist()\n",
    "    pred_start = pred_start.reshape(batch_size, -1).asnumpy()\n",
    "    pred_end = pred_end.reshape(batch_size, -1).asnumpy()\n",
    "    \n",
    "    for example_id, start, end in zip(example_ids, pred_start, pred_end):\n",
    "        all_results[example_id].append(PredResult(start=start, end=end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T01:45:32.623482Z",
     "start_time": "2019-06-14T01:45:32.578002Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Context: Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.\n",
      "\n",
      "Question: which nfl team represented the afc at super bowl 50 ?\n",
      "\n",
      "Top predictions: \n",
      "99.36% \t Denver Broncos\n",
      "0.23% \t The American Football Conference (AFC) champion Denver Broncos\n",
      "0.20% \t Broncos\n",
      "\n",
      "\n",
      "Context: Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.\n",
      "\n",
      "Question: which nfl team represented the nfc at super bowl 50 ?\n",
      "\n",
      "Top predictions: \n",
      "66.65% \t Carolina Panthers\n",
      "24.30% \t Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers\n",
      "7.42% \t Denver Broncos\n",
      "\n",
      "\n",
      "Context: Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.\n",
      "\n",
      "Question: where did super bowl 50 take place ?\n",
      "\n",
      "Top predictions: \n",
      "25.86% \t Levi's Stadium in the San Francisco Bay Area at Santa Clara, California\n",
      "23.11% \t Levi's Stadium\n",
      "17.88% \t San Francisco Bay Area at Santa Clara, California\n",
      "\n"
     ]
    }
   ],
   "source": [
    "qa_utils.predict(dataset, all_results, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's Train the Model\n",
    "\n",
    "Now we can put all the pieces together, and start fine-tuning the model with a few epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/gluon/parameter.py:887: UserWarning: Parameter 'bertforqa0_dense0_weight' is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  v.initialize(None, ctx, init, force_reinit=force_reinit)\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/gluon/parameter.py:887: UserWarning: Parameter 'bertforqa0_dense0_bias' is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  v.initialize(None, ctx, init, force_reinit=force_reinit)\n"
     ]
    }
   ],
   "source": [
    "# net = BertForQA(bert=bert_model)\n",
    "# nlp.utils.load_parameters(net, pretrained_bert_parameters, ctx=ctx,\n",
    "#                           ignore_extra=True, cast_dtype=True)\n",
    "net.span_classifier.initialize(init=mx.init.Normal(0.02), ctx=ctx)\n",
    "net.hybridize(static_alloc=True)\n",
    "\n",
    "loss_function = BertForQALoss()\n",
    "loss_function.hybridize(static_alloc=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy on SageMaker\n",
    "\n",
    "1. Preparing functions for inference \n",
    "2. Saving the model parameters\n",
    "3. Building a docker container with dependencies installed\n",
    "4. Launching a serving end-point with SageMaker SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preparing functions for inference\n",
    "\n",
    "Two functions: \n",
    "1. model_fn() to load model parameters\n",
    "2. transform_fn() to run model inference given an input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting code/serve.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile code/serve.py\n",
    "import collections, json, logging, warnings\n",
    "import multiprocessing as mp\n",
    "from functools import partial\n",
    "\n",
    "import gluonnlp as nlp\n",
    "import mxnet as mx\n",
    "from mxnet.gluon import Block, nn\n",
    "import bert\n",
    "from bert.data.qa import preprocess_dataset, SQuADTransform\n",
    "import bert_qa_evaluate\n",
    "\n",
    "\n",
    "\n",
    "class BertForQA(Block):\n",
    "    \"\"\"Model for SQuAD task with BERT.\n",
    "    The model feeds token ids and token type ids into BERT to get the\n",
    "    pooled BERT sequence representation, then apply a Dense layer for QA task.\n",
    "    Parameters\n",
    "    ----------\n",
    "    bert: BERTModel\n",
    "        Bidirectional encoder with transformer.\n",
    "    prefix : str or None\n",
    "        See document of `mx.gluon.Block`.\n",
    "    params : ParameterDict or None\n",
    "        See document of `mx.gluon.Block`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, bert, prefix=None, params=None):\n",
    "        super(BertForQA, self).__init__(prefix=prefix, params=params)\n",
    "        self.bert = bert\n",
    "        with self.name_scope():\n",
    "            self.span_classifier = nn.Dense(units=2, flatten=False)\n",
    "\n",
    "    def forward(self, inputs, token_types, valid_length=None):  # pylint: disable=arguments-differ\n",
    "        \"\"\"Generate the unnormalized score for the given the input sequences.\n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs : NDArray, shape (batch_size, seq_length)\n",
    "            Input words for the sequences.\n",
    "        token_types : NDArray, shape (batch_size, seq_length)\n",
    "            Token types for the sequences, used to indicate whether the word belongs to the\n",
    "            first sentence or the second one.\n",
    "        valid_length : NDArray or None, shape (batch_size,)\n",
    "            Valid length of the sequence. This is used to mask the padded tokens.\n",
    "        Returns\n",
    "        -------\n",
    "        outputs : NDArray\n",
    "            Shape (batch_size, seq_length, 2)\n",
    "        \"\"\"\n",
    "        bert_output = self.bert(inputs, token_types, valid_length)\n",
    "        output = self.span_classifier(bert_output)\n",
    "        return output\n",
    "    \n",
    "    \n",
    "def get_all_results(net, vocab, squadTransform, test_dataset, ctx = mx.cpu()):\n",
    "    all_results = collections.defaultdict(list)\n",
    "    \n",
    "    def _vocab_lookup(example_id, subwords, type_ids, length, start, end):\n",
    "        indices = vocab[subwords]\n",
    "        return example_id, indices, type_ids, length, start, end\n",
    "    \n",
    "    dev_data_transform, _ = preprocess_dataset(test_dataset, squadTransform)\n",
    "    dev_data_transform = dev_data_transform.transform(_vocab_lookup, lazy=False)\n",
    "    dev_dataloader = mx.gluon.data.DataLoader(dev_data_transform, batch_size=1, shuffle=False)\n",
    "    \n",
    "    for data in dev_dataloader:\n",
    "        example_ids, inputs, token_types, valid_length, _, _ = data\n",
    "        batch_size = inputs.shape[0]\n",
    "        output = net(inputs.astype('float32').as_in_context(ctx),\n",
    "                     token_types.astype('float32').as_in_context(ctx),\n",
    "                     valid_length.astype('float32').as_in_context(ctx))\n",
    "        pred_start, pred_end = mx.nd.split(output, axis=2, num_outputs=2)\n",
    "        example_ids = example_ids.asnumpy().tolist()\n",
    "        pred_start = pred_start.reshape(batch_size, -1).asnumpy()\n",
    "        pred_end = pred_end.reshape(batch_size, -1).asnumpy()\n",
    "\n",
    "        for example_id, start, end in zip(example_ids, pred_start, pred_end):\n",
    "            all_results[example_id].append(bert_qa_evaluate.PredResult(start=start, end=end))\n",
    "    return(all_results)\n",
    "\n",
    "\n",
    "def _test_example_transform(test_examples):\n",
    "    \"\"\"\n",
    "    Change test examples to a format like SQUAD data.\n",
    "    Parameters\n",
    "    ---------- \n",
    "    test_examples: a list of (question, context) tuple. \n",
    "        Example: [('Which NFL team represented the AFC at Super Bowl 50?',\n",
    "                 'Super Bowl 50 was an American football game ......),\n",
    "                  ('Where did Super Bowl 50 take place?',,\n",
    "                 'Super Bowl 50 was ......),\n",
    "                 ......]\n",
    "    Returns\n",
    "    ----------\n",
    "    test_examples_tuples : a list of SQUAD tuples\n",
    "    \"\"\"\n",
    "    test_examples_tuples = []\n",
    "    i = 0\n",
    "    for test in test_examples:\n",
    "        question, context = test[0], test[1]  # test.split(\" [CONTEXT] \")\n",
    "        tup = (i, \"\", question, context, [], [])\n",
    "        test_examples_tuples.append(tup)\n",
    "        i += 1\n",
    "    return(test_examples_tuples)\n",
    "\n",
    "\n",
    "def model_fn(model_dir = \"\", params_path = \"bert_qa-7eb11865.params\"):\n",
    "    \"\"\"\n",
    "    Load the gluon model. Called once when hosting service starts.\n",
    "    :param: model_dir The directory where model files are stored.\n",
    "    :return: a Gluon model, and the vocabulary\n",
    "    \"\"\"\n",
    "    bert_model, vocab = nlp.model.get_model('bert_12_768_12',\n",
    "                                        dataset_name='book_corpus_wiki_en_uncased',\n",
    "                                        use_classifier=False,\n",
    "                                        use_decoder=False,\n",
    "                                        use_pooler=False,\n",
    "                                        pretrained=False)\n",
    "    net = BertForQA(bert_model)\n",
    "    if len(model_dir) > 0:\n",
    "        params_path = model_dir + \"/\" +params_path\n",
    "    net.load_parameters(params_path, ctx=mx.cpu())\n",
    "    \n",
    "    tokenizer = nlp.data.BERTTokenizer(vocab,  lower=True)\n",
    "    transform = SQuADTransform(tokenizer, is_pad=False, is_training=False, do_lookup=False)\n",
    "    return net, vocab, transform\n",
    "\n",
    "\n",
    "def transform_fn(model, input_data, input_content_type=None, output_content_type=None):\n",
    "    \"\"\"\n",
    "    Transform a request using the Gluon model. Called once per request.\n",
    "    :param model: The Gluon model and the vocab\n",
    "    :param dataset: The request payload\n",
    "    \n",
    "        Example:\n",
    "        ## (example_id, [question, content], ques_cont_token_types, valid_length, _, _)\n",
    "\n",
    "\n",
    "        (2, \n",
    "        '56be4db0acb8001400a502ee', \n",
    "        'Where did Super Bowl 50 take place?', \n",
    "        \n",
    "        'Super Bowl 50 was an American football game to determine the champion of the National \n",
    "        Football League (NFL) for the 2015 season. The American Football Conference (AFC) \n",
    "        champion Denver Broncos defeated the National Football Conference (NFC) champion \n",
    "        Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played \n",
    "        on February 7, 2016, at Levi\\'s Stadium in the San Francisco Bay Area at Santa Clara, \n",
    "        California. As this was the 50th Super Bowl, the league emphasized the \"golden \n",
    "        anniversary\" with various gold-themed initiatives, as well as temporarily suspending \n",
    "        the tradition of naming each Super Bowl game with Roman numerals (under which the \n",
    "        game would have been known as \"Super Bowl L\"), so that the logo could prominently \n",
    "        feature the Arabic numerals 50.', \n",
    "        \n",
    "        ['Santa Clara, California', \"Levi's Stadium\", \"Levi's Stadium \n",
    "        in the San Francisco Bay Area at Santa Clara, California.\"], \n",
    "        \n",
    "        [403, 355, 355])\n",
    "\n",
    "    :param input_content_type: The request content type, assume json\n",
    "    :param output_content_type: The (desired) response content type, assume json\n",
    "    :return: response payload and content type.\n",
    "    \"\"\"\n",
    "    net, vocab, squadTransform = model\n",
    "#     data = input_data\n",
    "    data = json.loads(input_data)\n",
    "#     test_examples_tuples = [(i, \"\", question, content, [], [])]\n",
    "#     question, context = data #.split(\" [CONTEXT] \")\n",
    "#     tup = (0, \"\", question, context, [], [])\n",
    "    test_examples_tuples = _test_example_transform(data)\n",
    "    test_dataset = mx.gluon.data.SimpleDataset(test_examples_tuples)  # [tup]\n",
    "    all_results = get_all_results(net, vocab, squadTransform, test_dataset, ctx=mx.cpu())\n",
    "    all_predictions = collections.defaultdict(list) # collections.OrderedDict()\n",
    "    data_transform = test_dataset.transform(squadTransform._transform)\n",
    "    for features in data_transform:\n",
    "        f_id = features[0].example_id\n",
    "        results = all_results[f_id]\n",
    "        prediction, nbest = bert_qa_evaluate.predict(\n",
    "            features=features,\n",
    "            results=results,\n",
    "            tokenizer=nlp.data.BERTBasicTokenizer(vocab))        \n",
    "        nbest_prediction = [] \n",
    "        for i in range(3):\n",
    "            nbest_prediction.append('%.2f%% \\t %s'%(nbest[i][1] * 100, nbest[i][0]))\n",
    "        all_predictions[f_id] = nbest_prediction\n",
    "    response_body = json.dumps(all_predictions)\n",
    "    return response_body, output_content_type\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Saving the model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save parameters, model definition and vocabulary in a zip file\n",
    "\n",
    "# output_dir = \"model_outputs\"\n",
    "# if not os.path.exists(output_dir):\n",
    "#     os.mkdir(output_dir)\n",
    "\n",
    "with open('vocab.json', 'w') as f:\n",
    "    f.write(vocab.to_json())\n",
    "\n",
    "import tarfile\n",
    "with tarfile.open(\"model.tar.gz\", \"w:gz\") as tar:\n",
    "#     tar.add(\"Dockerfile\")\n",
    "    tar.add(\"code/serve.py\")\n",
    "    tar.add(\"bert/data/qa.py\")\n",
    "    tar.add(\"bert_qa_evaluate.py\")\n",
    "    tar.add(\"bert_qa-7eb11865.params\")\n",
    "    tar.add(\"vocab.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## test\n",
    "my_test_example_0 = ('Which NFL team represented the AFC at Super Bowl 50?',\n",
    " 'Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi\\'s Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.')\n",
    "\n",
    "my_test_example_1 = ('Where did Super Bowl 50 take place?',\n",
    " 'Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi\\'s Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.')\n",
    "\n",
    "my_test_example_concat_0 = \" [CONTEXT] \".join(my_test_example_0)\n",
    "my_test_example_concat_1 = \" [CONTEXT] \".join(my_test_example_1)\n",
    "my_test_examples = (my_test_example_0, my_test_example_1)\n",
    "\n",
    "\n",
    "## prepare test examples\n",
    "# with open('my_test_examples.json', 'w') as f:\n",
    "#     json.dump(my_test_examples, f)\n",
    "\n",
    "# mymodel = model_fn()\n",
    "# transform_fn(mymodel, my_test_examples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## test\n",
    "# !cp code/serve.py serve.py\n",
    "# import serve\n",
    "\n",
    "# # ## if change serve.py, needs to reload\n",
    "# # %load_ext autoreload\n",
    "# # %autoreload serve\n",
    "# import importlib\n",
    "# importlib.reload(serve)\n",
    "\n",
    "# mymodel = serve.model_fn()\n",
    "# serve.transform_fn(mymodel, 'my_test_examples.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Building a docker container with dependencies installed\n",
    "\n",
    "Let's prepare a docker container with all the dependencies required for model inference. Here we build a docker container based on the SageMaker MXNet inference container, and you can find the list of all available inference containers at https://docs.aws.amazon.com/sagemaker/latest/dg/pre-built-containers-frameworks-deep-learning.html\n",
    "\n",
    "Here we use local mode for demonstration purpose. To deploy on actual instances, you need to login into AWS elastic container registry (ECR) service, and push the container to ECR. \n",
    "\n",
    "```\n",
    "docker build -t $YOUR_EDR_DOCKER_TAG . -f Dockerfile\n",
    "$(aws ecr get-login --no-include-email --region $YOUR_REGION)\n",
    "docker push $YOUR_EDR_DOCKER_TAG\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile Dockerfile\n",
    "\n",
    "ARG REGION\n",
    "FROM 763104351884.dkr.ecr.$REGION.amazonaws.com/mxnet-inference:1.4.1-gpu-py3\n",
    "\n",
    "RUN pip install --upgrade --user --pre 'mxnet-mkl' 'https://github.com/dmlc/gluon-nlp/tarball/v0.9.x'\n",
    "\n",
    "RUN pip list | grep mxnet\n",
    "\n",
    "COPY *.py /opt/ml/model/code/\n",
    "COPY bert/data/qa.py /opt/ml/model/code/bert/data/\n",
    "COPY bert/bert_qa_evaluate.py /opt/ml/model/code/bert/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Docker login cmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !$(aws ecr get-login --no-include-email --region us-east-1 --registry-ids 763104351884)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon    843MB\n",
      "Step 1/7 : ARG REGION\n",
      "Step 2/7 : FROM 763104351884.dkr.ecr.$REGION.amazonaws.com/mxnet-inference:1.4.1-gpu-py3\n",
      " ---> d9dd4dcfe0c2\n",
      "Step 3/7 : RUN pip install --upgrade --user --pre 'mxnet-mkl' 'https://github.com/dmlc/gluon-nlp/tarball/v0.9.x'\n",
      " ---> Running in d7e4062a5fa9\n",
      "Collecting https://github.com/dmlc/gluon-nlp/tarball/v0.9.x\n",
      "  Downloading https://github.com/dmlc/gluon-nlp/tarball/v0.9.x\n",
      "Collecting mxnet-mkl\n",
      "  Downloading https://files.pythonhosted.org/packages/64/72/c5566aabde6ee0bda1f09d026603169a717dbd9f26f6be85ee2b4ed2cf03/mxnet_mkl-1.6.0b20191025-py2.py3-none-manylinux1_x86_64.whl (64.9MB)\n",
      "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/site-packages (from gluonnlp==0.9.0.dev0) (1.14.6)\n",
      "Requirement already satisfied, skipping upgrade: graphviz<0.9.0,>=0.8.1 in /usr/local/lib/python3.6/site-packages (from mxnet-mkl) (0.8.4)\n",
      "Requirement already satisfied, skipping upgrade: requests<3,>=2.20.0 in /usr/local/lib/python3.6/site-packages (from mxnet-mkl) (2.22.0)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /usr/local/lib/python3.6/site-packages (from requests<3,>=2.20.0->mxnet-mkl) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/site-packages (from requests<3,>=2.20.0->mxnet-mkl) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/site-packages (from requests<3,>=2.20.0->mxnet-mkl) (2019.6.16)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/site-packages (from requests<3,>=2.20.0->mxnet-mkl) (1.25.3)\n",
      "\u001b[91mERROR: mxnet-mkl 1.6.0b20191025 has requirement numpy<2.0.0,>1.16.0, but you'll have numpy 1.14.6 which is incompatible.\n",
      "\u001b[0mInstalling collected packages: mxnet-mkl, gluonnlp\n",
      "  Running setup.py install for gluonnlp: started\n",
      "    Running setup.py install for gluonnlp: finished with status 'done'\n",
      "Successfully installed gluonnlp-0.9.0.dev0 mxnet-mkl-1.6.0b20191025\n",
      "\u001b[91mWARNING: You are using pip version 19.1.1, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n",
      "\u001b[0mRemoving intermediate container d7e4062a5fa9\n",
      " ---> 769b4b6bf6a0\n",
      "Step 4/7 : RUN pip list | grep mxnet\n",
      " ---> Running in ac87cd61baef\n",
      "\u001b[91mWARNING: You are using pip version 19.1.1, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n",
      "\u001b[0mkeras-mxnet                       2.2.4.1       \n",
      "mxnet-cu100mkl                    1.4.1         \n",
      "mxnet-mkl                         1.6.0b20191025\n",
      "mxnet-model-server                1.0.4         \n",
      "sagemaker-mxnet-serving-container 1.0.0         \n",
      "Removing intermediate container ac87cd61baef\n",
      " ---> 3d6d8707b42e\n",
      "Step 5/7 : COPY *.py /opt/ml/model/code/\n",
      " ---> cd44c2970500\n",
      "Step 6/7 : COPY bert/data/qa.py /opt/ml/model/code/bert/data/\n",
      " ---> 578484d6fe69\n",
      "Step 7/7 : COPY bert/bert_qa_evaluate.py /opt/ml/model/code/bert/\n",
      " ---> 69248eb53d86\n",
      "Successfully built 69248eb53d86\n",
      "Successfully tagged my-docker:inference\n"
     ]
    }
   ],
   "source": [
    "!export REGION=$(wget -qO- http://169.254.169.254/latest/meta-data/placement/availability-zone) &&\\\n",
    " docker build --no-cache --build-arg REGION=${REGION::-1} -t my-docker:inference . -f Dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Launching a serving end-point with SageMaker SDK\n",
    "\n",
    "We create a MXNet model which can be deployed later, by specifying the docker image, and entry point for the inference code. If serve.py does not work, use dummy_hosting_module.py for debugging purpose. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.mxnet.model import MXNetModel\n",
    "sagemaker_model = MXNetModel(model_data='file:///home/ec2-user/SageMaker/ako2020-bert/tutorial/model.tar.gz',\n",
    "                             image='my-docker:inference', # docker images\n",
    "                             role=sagemaker.get_execution_role(), \n",
    "                             py_version='py3',            # python version\n",
    "                             entry_point='serve.py',\n",
    "                             source_dir='.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use 'local' mode to test our deployment code, where the inference happens on the current instance.\n",
    "If you are ready to deploy the model on a new instance, change the `instance_type` argument to values such as `ml.c4.xlarge`.\n",
    "\n",
    "Here we use 'local' mode for testing, for real instances use c5.2xlarge, p2.xlarge, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f48b0a467b8>: Failed to establish a new connection: [Errno 111] Connection refused',)': /ping\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f48b0a46550>: Failed to establish a new connection: [Errno 111] Connection refused',)': /ping\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f496f4fb630>: Failed to establish a new connection: [Errno 111] Connection refused',)': /ping\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /ping\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response',))': /ping\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /ping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attaching to tmpojzp5ky2_algo-1-k79et_1\n",
      "\u001b[36malgo-1-k79et_1  |\u001b[0m 2020-01-14 22:14:55,925 [INFO ] main com.amazonaws.ml.mms.ModelServer - \r\n",
      "\u001b[36malgo-1-k79et_1  |\u001b[0m MMS Home: /usr/local/lib/python3.6/site-packages\r\n",
      "\u001b[36malgo-1-k79et_1  |\u001b[0m Current directory: /\r\n",
      "\u001b[36malgo-1-k79et_1  |\u001b[0m Temp directory: /home/model-server/tmp\r\n",
      "\u001b[36malgo-1-k79et_1  |\u001b[0m Number of GPUs: 0\r\n",
      "\u001b[36malgo-1-k79et_1  |\u001b[0m Number of CPUs: 8\r\n",
      "\u001b[36malgo-1-k79et_1  |\u001b[0m Max heap size: 13646 M\r\n",
      "\u001b[36malgo-1-k79et_1  |\u001b[0m Python executable: /usr/local/bin/python3.6\r\n",
      "\u001b[36malgo-1-k79et_1  |\u001b[0m Config file: /etc/sagemaker-mms.properties\r\n",
      "\u001b[36malgo-1-k79et_1  |\u001b[0m Inference address: http://0.0.0.0:8080\r\n",
      "\u001b[36malgo-1-k79et_1  |\u001b[0m Management address: http://127.0.0.1:8081\r\n",
      "\u001b[36malgo-1-k79et_1  |\u001b[0m Model Store: /.sagemaker/mms/models\r\n",
      "\u001b[36malgo-1-k79et_1  |\u001b[0m Initial Models: ALL\r\n",
      "\u001b[36malgo-1-k79et_1  |\u001b[0m Log dir: /logs\r\n",
      "\u001b[36malgo-1-k79et_1  |\u001b[0m Metrics dir: /logs\r\n",
      "\u001b[36malgo-1-k79et_1  |\u001b[0m Netty threads: 0\r\n",
      "\u001b[36malgo-1-k79et_1  |\u001b[0m Netty client threads: 0\r\n",
      "\u001b[36malgo-1-k79et_1  |\u001b[0m Default workers per model: 8\r\n",
      "\u001b[36malgo-1-k79et_1  |\u001b[0m Blacklist Regex: N/A\r\n",
      "\u001b[36malgo-1-k79et_1  |\u001b[0m Maximum Response Size: 6553500\r\n",
      "\u001b[36malgo-1-k79et_1  |\u001b[0m Maximum Request Size: 6553500\r\n",
      "\u001b[36malgo-1-k79et_1  |\u001b[0m 2020-01-14 22:14:55,983 [INFO ] main com.amazonaws.ml.mms.wlm.ModelManager - Model model loaded.\r\n",
      "\u001b[36malgo-1-k79et_1  |\u001b[0m 2020-01-14 22:14:56,032 [INFO ] main com.amazonaws.ml.mms.ModelServer - Initialize Inference server with: EpollServerSocketChannel.\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /ping\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /ping\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /ping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36malgo-1-k79et_1  |\u001b[0m 2020-01-14 22:14:56,199 [INFO ] W-9001-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.mms.sock.9001\n",
      "\u001b[36malgo-1-k79et_1  |\u001b[0m 2020-01-14 22:14:56,199 [INFO ] W-9001-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - [PID]81\n",
      "\u001b[36malgo-1-k79et_1  |\u001b[0m 2020-01-14 22:14:56,203 [INFO ] W-9005-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.mms.sock.9005\n",
      "\u001b[36malgo-1-k79et_1  |\u001b[0m 2020-01-14 22:14:56,203 [INFO ] W-9003-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.mms.sock.9003\n",
      "\u001b[36malgo-1-k79et_1  |\u001b[0m 2020-01-14 22:14:56,204 [INFO ] W-9003-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - [PID]77\n",
      "\u001b[36malgo-1-k79et_1  |\u001b[0m 2020-01-14 22:14:56,204 [INFO ] W-9002-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.mms.sock.9002\n",
      "\u001b[36malgo-1-k79et_1  |\u001b[0m 2020-01-14 22:14:56,205 [INFO ] W-9004-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.mms.sock.9004\n",
      "\u001b[36malgo-1-k79et_1  |\u001b[0m 2020-01-14 22:14:56,205 [INFO ] W-9002-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - [PID]75\n",
      "\u001b[36malgo-1-k79et_1  |\u001b[0m 2020-01-14 22:14:56,205 [INFO ] W-9005-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - [PID]76\n",
      "\u001b[36malgo-1-k79et_1  |\u001b[0m 2020-01-14 22:14:56,205 [INFO ] W-9004-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - [PID]79\n",
      "\u001b[36malgo-1-k79et_1  |\u001b[0m 2020-01-14 22:14:56,207 [INFO ] W-9001-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - MXNet worker started.\n",
      "\u001b[36malgo-1-k79et_1  |\u001b[0m 2020-01-14 22:14:56,207 [INFO ] W-9001-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Python runtime: 3.6.8\n",
      "\u001b[36malgo-1-k79et_1  |\u001b[0m 2020-01-14 22:14:56,207 [INFO ] W-9004-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - MXNet worker started.\n",
      "\u001b[36malgo-1-k79et_1  |\u001b[0m 2020-01-14 22:14:56,207 [INFO ] W-9005-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - MXNet worker started.\n",
      "\u001b[36malgo-1-k79et_1  |\u001b[0m 2020-01-14 22:14:56,208 [INFO ] W-9004-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Python runtime: 3.6.8\n",
      "\u001b[36malgo-1-k79et_1  |\u001b[0m 2020-01-14 22:14:56,207 [INFO ] W-9002-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - MXNet worker started.\n",
      "\u001b[36malgo-1-k79et_1  |\u001b[0m 2020-01-14 22:14:56,208 [INFO ] W-9005-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Python runtime: 3.6.8\n",
      "\u001b[36malgo-1-k79et_1  |\u001b[0m 2020-01-14 22:14:56,208 [INFO ] W-9002-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Python runtime: 3.6.8\n",
      "\u001b[36malgo-1-k79et_1  |\u001b[0m 2020-01-14 22:14:56,208 [INFO ] W-9003-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - MXNet worker started.\n",
      "\u001b[36malgo-1-k79et_1  |\u001b[0m 2020-01-14 22:14:56,208 [INFO ] W-9003-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Python runtime: 3.6.8\n",
      "\u001b[36malgo-1-k79et_1  |\u001b[0m 2020-01-14 22:14:56,214 [INFO ] W-9004-model com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.mms.sock.9004\n",
      "\u001b[36malgo-1-k79et_1  |\u001b[0m 2020-01-14 22:14:56,214 [INFO ] W-9003-model com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.mms.sock.9003\n",
      "\u001b[36malgo-1-k79et_1  |\u001b[0m 2020-01-14 22:14:56,214 [INFO ] W-9002-model com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.mms.sock.9002\n",
      "\u001b[36malgo-1-k79et_1  |\u001b[0m 2020-01-14 22:14:56,214 [INFO ] W-9005-model com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.mms.sock.9005\n",
      "\u001b[36malgo-1-k79et_1  |\u001b[0m 2020-01-14 22:14:56,214 [INFO ] W-9001-model com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.mms.sock.9001\n",
      "\u001b[36malgo-1-k79et_1  |\u001b[0m 2020-01-14 22:14:56,229 [INFO ] W-9007-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.mms.sock.9007\n",
      "\u001b[36malgo-1-k79et_1  |\u001b[0m 2020-01-14 22:14:56,229 [INFO ] W-9007-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - [PID]84\n",
      "\u001b[36malgo-1-k79et_1  |\u001b[0m 2020-01-14 22:14:56,230 [INFO ] W-9007-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - MXNet worker started.\n",
      "\u001b[36malgo-1-k79et_1  |\u001b[0m 2020-01-14 22:14:56,230 [INFO ] W-9007-model com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.mms.sock.9007\n",
      "\u001b[36malgo-1-k79et_1  |\u001b[0m 2020-01-14 22:14:56,230 [INFO ] W-9007-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Python runtime: 3.6.8\n",
      "\u001b[36malgo-1-k79et_1  |\u001b[0m 2020-01-14 22:14:56,232 [INFO ] W-9006-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.mms.sock.9006\n",
      "\u001b[36malgo-1-k79et_1  |\u001b[0m 2020-01-14 22:14:56,232 [INFO ] W-9006-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - [PID]82\n",
      "\u001b[36malgo-1-k79et_1  |\u001b[0m 2020-01-14 22:14:56,232 [INFO ] W-9006-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - MXNet worker started.\n",
      "\u001b[36malgo-1-k79et_1  |\u001b[0m 2020-01-14 22:14:56,232 [INFO ] W-9006-model com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.mms.sock.9006\n",
      "\u001b[36malgo-1-k79et_1  |\u001b[0m 2020-01-14 22:14:56,232 [INFO ] W-9006-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Python runtime: 3.6.8\n",
      "\u001b[36malgo-1-k79et_1  |\u001b[0m 2020-01-14 22:14:56,261 [INFO ] W-9001-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.mms.sock.9001.\n",
      "\u001b[36malgo-1-k79et_1  |\u001b[0m 2020-01-14 22:14:56,261 [INFO ] W-9005-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.mms.sock.9005.\n",
      "\u001b[36malgo-1-k79et_1  |\u001b[0m 2020-01-14 22:14:56,261 [INFO ] W-9004-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.mms.sock.9004.\n",
      "\u001b[36malgo-1-k79et_1  |\u001b[0m 2020-01-14 22:14:56,262 [INFO ] main com.amazonaws.ml.mms.ModelServer - Inference API bind to: http://0.0.0.0:8080\n",
      "\u001b[36malgo-1-k79et_1  |\u001b[0m 2020-01-14 22:14:56,262 [INFO ] main com.amazonaws.ml.mms.ModelServer - Initialize Management server with: EpollServerSocketChannel.\n",
      "\u001b[36malgo-1-k79et_1  |\u001b[0m 2020-01-14 22:14:56,263 [INFO ] W-9007-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.mms.sock.9007.\n",
      "\u001b[36malgo-1-k79et_1  |\u001b[0m 2020-01-14 22:14:56,263 [INFO ] W-9006-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.mms.sock.9006.\n",
      "\u001b[36malgo-1-k79et_1  |\u001b[0m 2020-01-14 22:14:56,263 [INFO ] W-9002-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.mms.sock.9002.\n",
      "\u001b[36malgo-1-k79et_1  |\u001b[0m 2020-01-14 22:14:56,266 [INFO ] W-9003-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.mms.sock.9003.\n",
      "\u001b[36malgo-1-k79et_1  |\u001b[0m 2020-01-14 22:14:56,266 [INFO ] main com.amazonaws.ml.mms.ModelServer - Management API bind to: http://127.0.0.1:8081\n",
      "\u001b[36malgo-1-k79et_1  |\u001b[0m 2020-01-14 22:14:56,267 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.mms.sock.9000\n",
      "\u001b[36malgo-1-k79et_1  |\u001b[0m 2020-01-14 22:14:56,267 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - [PID]78\n",
      "\u001b[36malgo-1-k79et_1  |\u001b[0m 2020-01-14 22:14:56,267 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - MXNet worker started.\n",
      "\u001b[36malgo-1-k79et_1  |\u001b[0m 2020-01-14 22:14:56,267 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.mms.sock.9000\n",
      "\u001b[36malgo-1-k79et_1  |\u001b[0m 2020-01-14 22:14:56,267 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Python runtime: 3.6.8\n",
      "\u001b[36malgo-1-k79et_1  |\u001b[0m Model server started.\n",
      "\u001b[36malgo-1-k79et_1  |\u001b[0m 2020-01-14 22:14:56,269 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.mms.sock.9000.\n",
      "\u001b[36malgo-1-k79et_1  |\u001b[0m 2020-01-14 22:14:59,298 [INFO ] W-9005-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 3003\n",
      "\u001b[36malgo-1-k79et_1  |\u001b[0m 2020-01-14 22:14:59,342 [INFO ] W-9002-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 3050\n",
      "\u001b[36malgo-1-k79et_1  |\u001b[0m 2020-01-14 22:14:59,352 [INFO ] W-9003-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 3041\n",
      "\u001b[36malgo-1-k79et_1  |\u001b[0m 2020-01-14 22:14:59,358 [INFO ] W-9001-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 3047\n",
      "\u001b[36malgo-1-k79et_1  |\u001b[0m 2020-01-14 22:14:59,361 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 3058\n",
      "\u001b[36malgo-1-k79et_1  |\u001b[0m 2020-01-14 22:14:59,368 [INFO ] W-9004-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 3073\n",
      "\u001b[36malgo-1-k79et_1  |\u001b[0m 2020-01-14 22:14:59,404 [INFO ] W-9006-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 3101\n",
      "\u001b[36malgo-1-k79et_1  |\u001b[0m 2020-01-14 22:14:59,417 [INFO ] W-9007-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 3114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36malgo-1-k79et_1  |\u001b[0m 2020-01-14 22:15:01,263 [INFO ] pool-1-thread-9 ACCESS_LOG - /172.18.0.1:59384 \"GET /ping HTTP/1.1\" 200 12\r\n",
      "!"
     ]
    }
   ],
   "source": [
    "predictor = sagemaker_model.deploy(initial_instance_count=1, instance_type='local')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36malgo-1-k79et_1  |\u001b[0m 2020-01-14 22:17:46,083 [INFO ] W-9005-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Vocab file is not found. Downloading.\n",
      "\u001b[36malgo-1-k79et_1  |\u001b[0m 2020-01-14 22:17:46,083 [INFO ] W-9005-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Downloading /root/.mxnet/models/1579040266.0833917book_corpus_wiki_en_uncased-a6607397.zip from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/vocab/book_corpus_wiki_en_uncased-a6607397.zip...\n",
      "\u001b[36malgo-1-k79et_1  |\u001b[0m 2020-01-14 22:17:47,791 [INFO ] W-9005-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Done! Transform dataset costs 0.58 seconds.\n",
      "\u001b[36malgo-1-k79et_1  |\u001b[0m 2020-01-14 22:17:49,545 [INFO ] W-9005-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 3479\n",
      "\u001b[36malgo-1-k79et_1  |\u001b[0m 2020-01-14 22:17:49,546 [INFO ] W-9005-model ACCESS_LOG - /172.18.0.1:59402 \"POST /invocations HTTP/1.1\" 200 3483\n",
      "\n",
      "Prediction output: {'0': ['99.36% \\t Denver Broncos', '0.23% \\t The American Football Conference (AFC) champion Denver Broncos', '0.20% \\t Broncos'], '1': [\"25.86% \\t Levi's Stadium in the San Francisco Bay Area at Santa Clara, California\", \"23.11% \\t Levi's Stadium\", '17.88% \\t San Francisco Bay Area at Santa Clara, California']}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output = predictor.predict(my_test_examples)  \n",
    "print('\\nPrediction output: {}\\n\\n'.format(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Up\n",
    "\n",
    "Remove the endpoint after we are done. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Side notes (Do NOT Run!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget -qO- http://169.254.169.254/latest/meta-data/placement/availability-zone\n",
    "# !aws ecr get-login --no-include-email --region us-east-1 --registry-ids 763104351884\n",
    "# !docker pull 763104351884.dkr.ecr.us-east-1.amazonaws.com/mxnet-inference:1.6.0-gpu-py3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        (2, \n",
    "        ['[CLS]', 'where', 'did', 'super', 'bowl', '50', 'take', 'place', '?', \n",
    "        '[SEP]', 'super', 'bowl', '50', 'was', 'an', 'american', 'football', \n",
    "        'game', 'to', 'determine', 'the', 'champion', 'of', 'the', 'national', \n",
    "        'football', 'league', '(', 'nfl', ')', 'for', 'the', '2015', 'season', '.', \n",
    "        'the', 'american', 'football', 'conference', '(', 'afc', ')', 'champion', \n",
    "        'denver', 'broncos', 'defeated', 'the', 'national', 'football', 'conference', \n",
    "        '(', 'nfc', ')', 'champion', 'carolina', 'panthers', '24', '–', '10', 'to', \n",
    "        'earn', 'their', 'third', 'super', 'bowl', 'title', '.', 'the', 'game', 'was', \n",
    "        'played', 'on', 'february', '7', ',', '2016', ',', 'at', 'levi', \"'\", 's', 'stadium', \n",
    "        'in', 'the', 'san', 'francisco', 'bay', 'area', 'at', 'santa', 'clara', ',', \n",
    "        'california', '.', 'as', 'this', 'was', 'the', '50th', 'super', 'bowl', ',', \n",
    "        'the', 'league', 'emphasized', 'the', '\"', 'golden', 'anniversary', '\"', 'with', \n",
    "        'various', 'gold', '-', 'themed', 'initiatives', ',', 'as', 'well', 'as', \n",
    "        'temporarily', 'suspend', '##ing', 'the', 'tradition', 'of', 'naming', \n",
    "        'each', 'super', 'bowl', 'game', 'with', 'roman', 'nu', '##meral', '##s', \n",
    "        '(', 'under', 'which', 'the', 'game', 'would', 'have', 'been', 'known', \n",
    "        'as', '\"', 'super', 'bowl', 'l', '\"', ')', ',', 'so', 'that', 'the', 'logo', \n",
    "        'could', 'prominently', 'feature', 'the', 'arabic', 'nu', '##meral', '##s', \n",
    "        '50', '.', '[SEP]'], \n",
    "        \n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n",
    "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n",
    "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n",
    "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n",
    "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n",
    "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n",
    "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \n",
    "        \n",
    "        168, 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-7:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sagemaker/local/image.py\", line 611, in run\n",
      "    _stream_output(self.process)\n",
      "  File \"/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sagemaker/local/image.py\", line 672, in _stream_output\n",
      "    raise RuntimeError(\"Process exited with code: %s\" % exit_code)\n",
      "RuntimeError: Process exited with code: 1\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sagemaker/local/image.py\", line 616, in run\n",
      "    raise RuntimeError(msg)\n",
      "RuntimeError: Failed to run: ['docker-compose', '-f', '/tmp/tmp1xmzmqgn/docker-compose.yaml', 'up', '--build', '--abort-on-container-exit'], Process exited with code: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ## prepare dataset json\n",
    "# my_test_examples_tuples = test_example_transform(my_test_examples)\n",
    "# with open('test_examples_tuples.json', 'w') as f:\n",
    "#     json.dump(my_test_examples_tuples, f)\n",
    "\n",
    "# test_dataset[1]\n",
    "\n",
    "\n",
    "\n",
    "# with open('dataset.json') as data_file:    \n",
    "#     data = json.load(data_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "kernelspec": {"display_name": "conda_mxnet_p36","language": "python", "name": "conda_mxnet_p36"},"language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
