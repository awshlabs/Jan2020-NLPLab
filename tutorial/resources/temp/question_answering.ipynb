{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hands-on: Training and deploying Question Answering with BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-trained language representations have been shown to improve many downstream NLP tasks such as question answering, and natural language inference. Devlin, Jacob, et al proposed BERT [1] (Bidirectional Encoder Representations from Transformers), which fine-tunes deep bidirectional representations on a wide range of tasks with minimal task-specific parameters, and obtained state- of-the-art results.\n",
    "\n",
    "In this tutorial, we will focus on adapting the BERT model for the question answering task on the SQuAD dataset. Specifically, we will:\n",
    "\n",
    "- understand how to pre-process the SQuAD dataset to leverage the learnt representation in BERT,\n",
    "- adapt the BERT model to the question answering task, and\n",
    "- load a trained model to perform inference on the SQuAD dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sagemaker configuration\n",
    "\n",
    "This notebook requires mxnet-cu101 >= 1.6.0b20191102, gluonnlp >= 0.8.1\n",
    "We can create a sagemaker notebook instance with the lifecycle configuration file: sagemaker-lifecycle.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# One time script\n",
    "# !bash sagemaker-lifecycle.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip list | grep mxnet\n",
    "!pip list | grep gluonnlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Load MXNet and GluonNLP\n",
    "\n",
    "We first import the libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse, collections, time, logging\n",
    "import json\n",
    "import os\n",
    "import io\n",
    "import copy\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import gluonnlp as nlp\n",
    "import mxnet as mx\n",
    "import bert\n",
    "import qa_utils\n",
    "\n",
    "from gluonnlp.data import SQuAD\n",
    "from bert.model.qa import BertForQALoss, BertForQA\n",
    "from bert.data.qa import SQuADTransform, preprocess_dataset\n",
    "from bert.bert_qa_evaluate import get_F1_EM, predict, PredResult\n",
    "\n",
    "# Hyperparameters\n",
    "parser = argparse.ArgumentParser('BERT finetuning')\n",
    "parser.add_argument('--epochs', type=int, default=3)\n",
    "parser.add_argument('--batch_size', default=32)\n",
    "parser.add_argument('--num_epochs', default=1)\n",
    "parser.add_argument('--lr', default=5e-5)\n",
    "args = parser.parse_args([])\n",
    "\n",
    "epochs = args.epochs\n",
    "batch_size = args.batch_size\n",
    "num_epochs = args.num_epochs\n",
    "lr = args.lr\n",
    "\n",
    "# output_dir = args.output_dir\n",
    "# if not os.path.exists(output_dir):\n",
    "#     os.mkdir(output_dir)\n",
    "# test_batch_size = args.test_batch_size\n",
    "# optimizer = args.optimizer\n",
    "# accumulate = args.accumulate\n",
    "# warmup_ratio = args.warmup_ratio\n",
    "# log_interval = args.log_interval\n",
    "# max_seq_length = args.max_seq_length\n",
    "# doc_stride = args.doc_stride\n",
    "# max_query_length = args.max_query_length\n",
    "# n_best_size = args.n_best_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Inspect the SQuAD Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Then we take a look at the Stanford Question Answering Dataset (SQuAD). The dataset can be downloaded using the `nlp.data.SQuAD` API. In this tutorial, we create a small dataset with 3 samples from the SQuAD dataset for demonstration purpose.\n",
    "\n",
    "The question answering task on the SQuAD dataset is setup the following way. For each sample in the dataset, a context is provided. The context is usually a long paragraph which contains lots of information. Then a question asked based on the context. The goal is to find the text span in the context that answers the question in the sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T01:45:27.555133Z",
     "start_time": "2019-06-14T01:45:27.418706Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "full_data = nlp.data.SQuAD(segment='dev', version='1.1')\n",
    "# loading a subset of the dev set of SQuAD\n",
    "num_target_samples = 3\n",
    "target_samples = [full_data[i] for i in range(num_target_samples)]\n",
    "dataset = mx.gluon.data.SimpleDataset(target_samples)\n",
    "print('Number of samples in the created dataset subsampled from SQuAD = %d'%len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_samples[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the above example is the structure of the SQuAD dataset. Here, the question index is 2, the context index is 3, the answer index is 4, and the answer position index is 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T01:45:27.560564Z",
     "start_time": "2019-06-14T01:45:27.557274Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "question_idx = 2\n",
    "context_idx = 3\n",
    "answer_idx = 4\n",
    "answer_pos_idx = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Let's take a look at a sample from the dataset. In this sample, the question is about the location of the game, with a description about the Super Bowl 50 game as the context. Note that three different answer spans are correct for this question, and they start from index 403, 355 and 355 in the context respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T01:45:27.567303Z",
     "start_time": "2019-06-14T01:45:27.562425Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "sample = dataset[2]\n",
    "print('\\nContext:\\n')\n",
    "print(sample[context_idx])\n",
    "print(\"\\nQuestion\")\n",
    "print(sample[question_idx])\n",
    "print(\"\\nCorrect Answer Spans\")\n",
    "print(sample[answer_idx])\n",
    "print(\"\\nAnswer Span Start Indices:\")\n",
    "print(sample[answer_pos_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Data Pre-processing for QA with BERT\n",
    "\n",
    "Recall that during BERT pre-training, it takes a sentence pair as the input, separated by the 'SEP' special token. For SQuAD, we can feed the context-question pair as the sentence pair input. To use BERT to predict the starting and ending span of the answer, we can add a classification layer for each token in the context texts, to predict if a token is the start or the end of the answer span. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T01:30:12.299493Z",
     "start_time": "2019-06-14T01:30:12.183419Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![qa](natural_language_understanding/qa.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "In the next few code blocks, we will work on pre-processing the samples in the SQuAD dataset in the desired format with these special separators. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Pre-trained BERT Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "First, let's use the *get_model* API in GluonNLP to get the model definition for BERT, and the vocabulary used for the BERT model. Note that we discard the pooler and classifier layers used for the next sentence prediction task, as well as the decoder layers for the masked language model task during the BERT pre-training phase. These layers are not useful for predicting the starting and ending indices of the answer span.\n",
    "\n",
    "The list of pre-trained BERT models available in GluonNLP can be found [here](http://gluon-nlp.mxnet.io/model_zoo/bert/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T01:45:27.715444Z",
     "start_time": "2019-06-14T01:45:27.569118Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "bert_model, vocab = nlp.model.get_model('bert_12_768_12',\n",
    "                                        dataset_name='book_corpus_wiki_en_uncased',\n",
    "                                        use_classifier=False,\n",
    "                                        use_decoder=False,\n",
    "                                        use_pooler=False,\n",
    "                                        pretrained=False)\n",
    "with open('vocab.json', 'w') as f:\n",
    "    f.write(vocab.to_json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Note that there are several special tokens in the vocabulary for BERT. In particular, the `[SEP]` token is used for separating the sentence pairs, and the `[CLS]` token is added at the beginning of the sentence pairs. They will be used to pre-process the SQuAD dataset later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T01:45:27.720137Z",
     "start_time": "2019-06-14T01:45:27.717192Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The second step is to process the samples using the same tokenizer used for BERT, which is provided as the `BERTTokenizer` API in GluonNLP. Note that instead of word level and character level representation, BERT uses subwords to represent a word, separated `##`. \n",
    "\n",
    "In the following example, the word `suspending` is tokenized as two subwords (`suspend` and `##ing`), and `numerals` is tokenized as three subwords (`nu`, `##meral`, `##s`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T01:45:27.731724Z",
     "start_time": "2019-06-14T01:45:27.721690Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = nlp.data.BERTTokenizer(vocab=vocab, lower=True)\n",
    "\n",
    "tokenizer(\"as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Sentence Pair Composition\n",
    "\n",
    "With the tokenizer inplace, we are ready to process the question-context texts and compose sentence pairs. The functionality is available via the `SQuADTransform` API. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T01:45:27.897684Z",
     "start_time": "2019-06-14T01:45:27.734029Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "transform = bert.data.qa.SQuADTransform(tokenizer, is_pad=False, is_training=False, do_lookup=False)\n",
    "dev_data_transform, _ = preprocess_dataset(dataset, transform)\n",
    "logging.info('The number of examples after preprocessing:{}'.format(len(dev_data_transform)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's take a look at the sample after the transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T01:45:27.904353Z",
     "start_time": "2019-06-14T01:45:27.899992Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sample = dev_data_transform[2]\n",
    "print('\\nsegment type: \\n' + str(sample[2]))\n",
    "print('\\ntext length: ' + str(sample[3]))\n",
    "print('\\nsentence pair: \\n' + str(sample[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Vocabulary Lookup\n",
    "\n",
    "Finally, we convert the transformed texts to subword indices, which are used to contructor NDArrays as the inputs to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T01:45:27.910853Z",
     "start_time": "2019-06-14T01:45:27.906127Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def vocab_lookup(example_id, subwords, type_ids, length, start, end):\n",
    "    indices = vocab[subwords]\n",
    "    return example_id, indices, type_ids, length, start, end\n",
    "\n",
    "dev_data_transform = dev_data_transform.transform(vocab_lookup, lazy=False)\n",
    "print(dev_data_transform[2][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model Definition\n",
    "\n",
    "After the data is processed, we can define the model that uses the representation produced by BERT for predicting the starting and ending positions of the answer span."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We download a BERT model trained on the SQuAD dataset, prepare the dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T01:45:32.221383Z",
     "start_time": "2019-06-14T01:45:27.922825Z"
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "net = BertForQA(bert_model)\n",
    "ctx = mx.gpu(0)\n",
    "ckpt = qa_utils.download_qa_ckpt()\n",
    "net.load_parameters(ckpt, ctx=ctx)\n",
    "\n",
    "batch_size = 1\n",
    "dev_dataloader = mx.gluon.data.DataLoader(\n",
    "    dev_data_transform, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T01:45:32.575976Z",
     "start_time": "2019-06-14T01:45:32.223336Z"
    }
   },
   "outputs": [],
   "source": [
    "all_results = collections.defaultdict(list)\n",
    "\n",
    "total_num = 0\n",
    "for data in dev_dataloader:\n",
    "    example_ids, inputs, token_types, valid_length, _, _ = data\n",
    "    total_num += len(inputs)\n",
    "    batch_size = inputs.shape[0]\n",
    "    output = net(inputs.astype('float32').as_in_context(ctx),\n",
    "                               token_types.astype('float32').as_in_context(ctx),\n",
    "                               valid_length.astype('float32').as_in_context(ctx))\n",
    "    pred_start, pred_end = mx.nd.split(output, axis=2, num_outputs=2)\n",
    "    example_ids = example_ids.asnumpy().tolist()\n",
    "    pred_start = pred_start.reshape(batch_size, -1).asnumpy()\n",
    "    pred_end = pred_end.reshape(batch_size, -1).asnumpy()\n",
    "    \n",
    "    for example_id, start, end in zip(example_ids, pred_start, pred_end):\n",
    "        all_results[example_id].append(PredResult(start=start, end=end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T01:45:32.623482Z",
     "start_time": "2019-06-14T01:45:32.578002Z"
    }
   },
   "outputs": [],
   "source": [
    "qa_utils.predict(dataset, all_results, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's Train the Model\n",
    "\n",
    "Now we can put all the pieces together, and start fine-tuning the model with a few epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net = BertForQA(bert=bert_model)\n",
    "# nlp.utils.load_parameters(net, pretrained_bert_parameters, ctx=ctx,\n",
    "#                           ignore_extra=True, cast_dtype=True)\n",
    "net.span_classifier.initialize(init=mx.init.Normal(0.02), ctx=ctx)\n",
    "net.hybridize(static_alloc=True)\n",
    "\n",
    "loss_function = BertForQALoss()\n",
    "loss_function.hybridize(static_alloc=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy on SageMaker\n",
    "\n",
    "1. Preparing functions for inference \n",
    "2. Saving the model parameters\n",
    "3. Building a docker container with dependencies installed\n",
    "4. Launching a serving end-point with SageMaker SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preparing functions for inference\n",
    "\n",
    "Two functions: \n",
    "1. model_fn() to load model parameters\n",
    "2. transform_fn() to run model inference given an input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile code/serve.py\n",
    "import collections, json, logging, warnings\n",
    "import multiprocessing as mp\n",
    "from functools import partial\n",
    "\n",
    "import gluonnlp as nlp\n",
    "import mxnet as mx\n",
    "from mxnet.gluon import Block, nn\n",
    "import bert\n",
    "from bert.data.qa import preprocess_dataset, SQuADTransform\n",
    "import bert_qa_evaluate\n",
    "\n",
    "\n",
    "\n",
    "class BertForQA(Block):\n",
    "    \"\"\"Model for SQuAD task with BERT.\n",
    "    The model feeds token ids and token type ids into BERT to get the\n",
    "    pooled BERT sequence representation, then apply a Dense layer for QA task.\n",
    "    Parameters\n",
    "    ----------\n",
    "    bert: BERTModel\n",
    "        Bidirectional encoder with transformer.\n",
    "    prefix : str or None\n",
    "        See document of `mx.gluon.Block`.\n",
    "    params : ParameterDict or None\n",
    "        See document of `mx.gluon.Block`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, bert, prefix=None, params=None):\n",
    "        super(BertForQA, self).__init__(prefix=prefix, params=params)\n",
    "        self.bert = bert\n",
    "        with self.name_scope():\n",
    "            self.span_classifier = nn.Dense(units=2, flatten=False)\n",
    "\n",
    "    def forward(self, inputs, token_types, valid_length=None):  # pylint: disable=arguments-differ\n",
    "        \"\"\"Generate the unnormalized score for the given the input sequences.\n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs : NDArray, shape (batch_size, seq_length)\n",
    "            Input words for the sequences.\n",
    "        token_types : NDArray, shape (batch_size, seq_length)\n",
    "            Token types for the sequences, used to indicate whether the word belongs to the\n",
    "            first sentence or the second one.\n",
    "        valid_length : NDArray or None, shape (batch_size,)\n",
    "            Valid length of the sequence. This is used to mask the padded tokens.\n",
    "        Returns\n",
    "        -------\n",
    "        outputs : NDArray\n",
    "            Shape (batch_size, seq_length, 2)\n",
    "        \"\"\"\n",
    "        bert_output = self.bert(inputs, token_types, valid_length)\n",
    "        output = self.span_classifier(bert_output)\n",
    "        return output\n",
    "    \n",
    "    \n",
    "def get_all_results(net, vocab, squadTransform, test_dataset, ctx = mx.cpu()):\n",
    "    all_results = collections.defaultdict(list)\n",
    "    \n",
    "    def _vocab_lookup(example_id, subwords, type_ids, length, start, end):\n",
    "        indices = vocab[subwords]\n",
    "        return example_id, indices, type_ids, length, start, end\n",
    "    \n",
    "    dev_data_transform, _ = preprocess_dataset(test_dataset, squadTransform)\n",
    "    dev_data_transform = dev_data_transform.transform(_vocab_lookup, lazy=False)\n",
    "    dev_dataloader = mx.gluon.data.DataLoader(dev_data_transform, batch_size=1, shuffle=False)\n",
    "    \n",
    "    for data in dev_dataloader:\n",
    "        example_ids, inputs, token_types, valid_length, _, _ = data\n",
    "        batch_size = inputs.shape[0]\n",
    "        output = net(inputs.astype('float32').as_in_context(ctx),\n",
    "                     token_types.astype('float32').as_in_context(ctx),\n",
    "                     valid_length.astype('float32').as_in_context(ctx))\n",
    "        pred_start, pred_end = mx.nd.split(output, axis=2, num_outputs=2)\n",
    "        example_ids = example_ids.asnumpy().tolist()\n",
    "        pred_start = pred_start.reshape(batch_size, -1).asnumpy()\n",
    "        pred_end = pred_end.reshape(batch_size, -1).asnumpy()\n",
    "\n",
    "        for example_id, start, end in zip(example_ids, pred_start, pred_end):\n",
    "            all_results[example_id].append(bert_qa_evaluate.PredResult(start=start, end=end))\n",
    "    return(all_results)\n",
    "\n",
    "\n",
    "def _test_example_transform(test_examples):\n",
    "    \"\"\"\n",
    "    Change test examples to a format like SQUAD data.\n",
    "    Parameters\n",
    "    ---------- \n",
    "    test_examples: a list of (question, context) tuple. \n",
    "        Example: [('Which NFL team represented the AFC at Super Bowl 50?',\n",
    "                 'Super Bowl 50 was an American football game ......),\n",
    "                  ('Where did Super Bowl 50 take place?',,\n",
    "                 'Super Bowl 50 was ......),\n",
    "                 ......]\n",
    "    Returns\n",
    "    ----------\n",
    "    test_examples_tuples : a list of SQUAD tuples\n",
    "    \"\"\"\n",
    "    test_examples_tuples = []\n",
    "    i = 0\n",
    "    for test in test_examples:\n",
    "        question, context = test[0], test[1]  # test.split(\" [CONTEXT] \")\n",
    "        tup = (i, \"\", question, context, [], [])\n",
    "        test_examples_tuples.append(tup)\n",
    "        i += 1\n",
    "    return(test_examples_tuples)\n",
    "\n",
    "\n",
    "def model_fn(model_dir = \"\", params_path = \"bert_qa-7eb11865.params\"):\n",
    "    \"\"\"\n",
    "    Load the gluon model. Called once when hosting service starts.\n",
    "    :param: model_dir The directory where model files are stored.\n",
    "    :return: a Gluon model, and the vocabulary\n",
    "    \"\"\"\n",
    "    bert_model, vocab = nlp.model.get_model('bert_12_768_12',\n",
    "                                        dataset_name='book_corpus_wiki_en_uncased',\n",
    "                                        use_classifier=False,\n",
    "                                        use_decoder=False,\n",
    "                                        use_pooler=False,\n",
    "                                        pretrained=False)\n",
    "    net = BertForQA(bert_model)\n",
    "    if len(model_dir) > 0:\n",
    "        params_path = model_dir + \"/\" +params_path\n",
    "    net.load_parameters(params_path, ctx=mx.cpu())\n",
    "    \n",
    "    tokenizer = nlp.data.BERTTokenizer(vocab,  lower=True)\n",
    "    transform = SQuADTransform(tokenizer, is_pad=False, is_training=False, do_lookup=False)\n",
    "    return net, vocab, transform\n",
    "\n",
    "\n",
    "def transform_fn(model, input_data, input_content_type=None, output_content_type=None):\n",
    "    \"\"\"\n",
    "    Transform a request using the Gluon model. Called once per request.\n",
    "    :param model: The Gluon model and the vocab\n",
    "    :param dataset: The request payload\n",
    "    \n",
    "        Example:\n",
    "        ## (example_id, [question, content], ques_cont_token_types, valid_length, _, _)\n",
    "\n",
    "\n",
    "        (2, \n",
    "        '56be4db0acb8001400a502ee', \n",
    "        'Where did Super Bowl 50 take place?', \n",
    "        \n",
    "        'Super Bowl 50 was an American football game to determine the champion of the National \n",
    "        Football League (NFL) for the 2015 season. The American Football Conference (AFC) \n",
    "        champion Denver Broncos defeated the National Football Conference (NFC) champion \n",
    "        Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played \n",
    "        on February 7, 2016, at Levi\\'s Stadium in the San Francisco Bay Area at Santa Clara, \n",
    "        California. As this was the 50th Super Bowl, the league emphasized the \"golden \n",
    "        anniversary\" with various gold-themed initiatives, as well as temporarily suspending \n",
    "        the tradition of naming each Super Bowl game with Roman numerals (under which the \n",
    "        game would have been known as \"Super Bowl L\"), so that the logo could prominently \n",
    "        feature the Arabic numerals 50.', \n",
    "        \n",
    "        ['Santa Clara, California', \"Levi's Stadium\", \"Levi's Stadium \n",
    "        in the San Francisco Bay Area at Santa Clara, California.\"], \n",
    "        \n",
    "        [403, 355, 355])\n",
    "\n",
    "    :param input_content_type: The request content type, assume json\n",
    "    :param output_content_type: The (desired) response content type, assume json\n",
    "    :return: response payload and content type.\n",
    "    \"\"\"\n",
    "    net, vocab, squadTransform = model\n",
    "    data = json.loads(input_data)\n",
    "    test_examples_tuples = _test_example_transform(data)\n",
    "    test_dataset = mx.gluon.data.SimpleDataset(test_examples_tuples)  # [tup]\n",
    "    all_results = get_all_results(net, vocab, squadTransform, test_dataset, ctx=mx.cpu())\n",
    "    all_predictions = collections.defaultdict(list) # collections.OrderedDict()\n",
    "    data_transform = test_dataset.transform(squadTransform._transform)\n",
    "    for features in data_transform:\n",
    "        f_id = features[0].example_id\n",
    "        results = all_results[f_id]\n",
    "        prediction, nbest = bert_qa_evaluate.predict(\n",
    "            features=features,\n",
    "            results=results,\n",
    "            tokenizer=nlp.data.BERTBasicTokenizer(vocab))        \n",
    "        nbest_prediction = [] \n",
    "        for i in range(3):\n",
    "            nbest_prediction.append('%.2f%% \\t %s'%(nbest[i][1] * 100, nbest[i][0]))\n",
    "        all_predictions[f_id] = nbest_prediction\n",
    "    response_body = json.dumps(all_predictions)\n",
    "    return response_body, output_content_type\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Saving the model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save parameters, model definition and vocabulary in a zip file\n",
    "\n",
    "# output_dir = \"model_outputs\"\n",
    "# if not os.path.exists(output_dir):\n",
    "#     os.mkdir(output_dir)\n",
    "\n",
    "with open('vocab.json', 'w') as f:\n",
    "    f.write(vocab.to_json())\n",
    "\n",
    "import tarfile\n",
    "with tarfile.open(\"model.tar.gz\", \"w:gz\") as tar:\n",
    "#     tar.add(\"Dockerfile\")\n",
    "    tar.add(\"code/serve.py\")\n",
    "    tar.add(\"bert/data/qa.py\")\n",
    "    tar.add(\"bert_qa_evaluate.py\")\n",
    "    tar.add(\"bert_qa-7eb11865.params\")\n",
    "    tar.add(\"vocab.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## test\n",
    "my_test_example_0 = ('Which NFL team represented the AFC at Super Bowl 50?',\n",
    " 'Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi\\'s Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.')\n",
    "\n",
    "my_test_example_1 = ('Where did Super Bowl 50 take place?',\n",
    " 'Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi\\'s Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.')\n",
    "\n",
    "my_test_example_concat_0 = \" [CONTEXT] \".join(my_test_example_0)\n",
    "my_test_example_concat_1 = \" [CONTEXT] \".join(my_test_example_1)\n",
    "my_test_examples = (my_test_example_0, my_test_example_1)\n",
    "\n",
    "\n",
    "## prepare test examples\n",
    "# with open('my_test_examples.json', 'w') as f:\n",
    "#     json.dump(my_test_examples, f)\n",
    "\n",
    "# mymodel = model_fn()\n",
    "# transform_fn(mymodel, my_test_examples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Building a docker container with dependencies installed\n",
    "\n",
    "Let's prepare a docker container with all the dependencies required for model inference. Here we build a docker container based on the SageMaker MXNet inference container, and you can find the list of all available inference containers at https://docs.aws.amazon.com/sagemaker/latest/dg/pre-built-containers-frameworks-deep-learning.html\n",
    "\n",
    "Here we use local mode for demonstration purpose. To deploy on actual instances, you need to login into AWS elastic container registry (ECR) service, and push the container to ECR. \n",
    "\n",
    "```\n",
    "docker build -t $YOUR_EDR_DOCKER_TAG . -f Dockerfile\n",
    "$(aws ecr get-login --no-include-email --region $YOUR_REGION)\n",
    "docker push $YOUR_EDR_DOCKER_TAG\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Dockerfile\n",
    "\n",
    "ARG REGION\n",
    "FROM 763104351884.dkr.ecr.$REGION.amazonaws.com/mxnet-inference:1.4.1-gpu-py3\n",
    "\n",
    "RUN pip install --upgrade --user --pre 'mxnet-mkl' 'https://github.com/dmlc/gluon-nlp/tarball/v0.9.x'\n",
    "\n",
    "RUN pip list | grep mxnet\n",
    "\n",
    "COPY *.py /opt/ml/model/code/\n",
    "COPY bert/data/qa.py /opt/ml/model/code/bert/data/\n",
    "COPY bert/bert_qa_evaluate.py /opt/ml/model/code/bert/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export REGION=$(wget -qO- http://169.254.169.254/latest/meta-data/placement/availability-zone) &&\\\n",
    " docker build --no-cache --build-arg REGION=${REGION::-1} -t my-docker:inference . -f Dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Launching a serving end-point with SageMaker SDK\n",
    "\n",
    "We create a MXNet model which can be deployed later, by specifying the docker image, and entry point for the inference code. If serve.py does not work, use dummy_hosting_module.py for debugging purpose. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.mxnet.model import MXNetModel\n",
    "sagemaker_model = MXNetModel(model_data='file:///home/ec2-user/SageMaker/ako2020-bert/tutorial/model.tar.gz',\n",
    "                             image='my-docker:inference', # docker images\n",
    "                             role=sagemaker.get_execution_role(), \n",
    "                             py_version='py3',            # python version\n",
    "                             entry_point='serve.py',\n",
    "                             source_dir='.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use 'local' mode to test our deployment code, where the inference happens on the current instance.\n",
    "If you are ready to deploy the model on a new instance, change the `instance_type` argument to values such as `ml.c4.xlarge`.\n",
    "\n",
    "Here we use 'local' mode for testing, for real instances use c5.2xlarge, p2.xlarge, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = sagemaker_model.deploy(initial_instance_count=1, instance_type='local')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output = predictor.predict(my_test_examples)  \n",
    "print('\\nPrediction output: {}\\n\\n'.format(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Up\n",
    "\n",
    "Remove the endpoint after we are done. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "Amazon SageMaker Developer Guide\n",
    "    https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-dg.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
